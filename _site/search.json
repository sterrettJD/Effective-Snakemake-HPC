[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Effective use of Snakemake for workflow management on high performance compute clusters",
    "section": "",
    "text": "Overview\nWelcome! This is a learning resource on how to effectively use the workflow management tool Snakemake to make your life easier when processing large amounts of data on high performance compute clusters.\n\n\nGoals\nHopefully, this resource will help you:\n\nUnderstand why workflow management tools are useful\nCreate generalizable workflows with Snakemake\nCreate reproducible workflows with Snakemake\nCreate Snakemake workflows that run on compute clusters in an efficient manner (parallelized job submission and resource management)\n\n\n\nApplications to shotgun metagenomics\nSome examples will be focused on dealing with high-throughput sequencing data, such as metagenomics, since this is what I (and the original audience of this resource) work with on a daily basis. However, the contents and principles still apply to other data types.\nA primer on shotgun metagenomic data:\n\nThe Fastq format is how we store sequencing data. It’s a (sometimes compressed) text file with the genome sequence and quality of the sequence.\nIf any of these concepts are unfamiliar to your, or if they spark an interest in working with metagenomic data:\n\nThomas Sharpton has written An introduction to the analysis of shotgun metagenomic data, which is a nice overview of many data processing steps.\nAdditionally, the Metagenomics Wiki explains many of the tools, as well as how to use them.\n\n\n\n\nApplications to 16S rDNA data\nOther examples may be based on the analysis of 16S rDNA sequencing data which is commonly performed for microbiome profiling and is what the other contributor to this resource works with on a daily basis. Once again, don’t fret if you’ve never encountered 16S rDNA data, the basic principles of Snakemake can be applied in a variety of situations.\nA brief background on 16S rDNA data:\n\nLike shotgun metagenomics, the sequencing data is stored in Fastq format as a compressed file.\nThese fastq.gz files are then put into microbiome profiling programs such as Qiime2 or mothur."
  },
  {
    "objectID": "Snakemake-Essentials.html#examples-1",
    "href": "Snakemake-Essentials.html#examples-1",
    "title": "Snakemake Essentials",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "Snakemake-Essentials.html",
    "href": "Snakemake-Essentials.html",
    "title": "Snakemake Essentials",
    "section": "",
    "text": "Installation instructions can be found on the Snakemake documentation page here. In short, you want to install conda and mamba-forge, then run:\nmamba create -c conda-forge -c bioconda -n snakemake_env snakemake\nAlternatively, if you want to install Snakemake in an environment you already have set up, first activate that environment, then run:\npip install snakemake\nThis is especially important if you will be using more than one conda environment in your pipeline."
  },
  {
    "objectID": "Snakemake-Essentials.html#what-is-rule-all",
    "href": "Snakemake-Essentials.html#what-is-rule-all",
    "title": "Snakemake Essentials",
    "section": "What is “rule all”?",
    "text": "What is “rule all”?\nAt this point, you may be asking “Wait! You said I should have a rule for each step! What’s this rule all mess??”\nrule all is how we specify the output files we want and its located at the top of your snakefile. We specify our target files using the input to rule all since Snakemake will automatically start at the rule at the top of your snakefile. In a scientific case, consider these the input to the paper you’ll write from your analysis.\nSnakemake is tracking what rules need to be run in order to generate the inputs for other rules, so it will track that - rule run_multiQC’s outputs -&gt; rule all’s inputs - rule run_fastQC’s outputs -&gt; rule run_multiQC’s inputs - rule trim_fastq’s outputs -&gt; rule run_fastQC’s inputs\nTherefore, it knows that in order to have the final files, it will use rule trim_fastq -&gt; rule run_fastQC -&gt; rule run_multiQC -&gt; rule all\n\n\n\n\nflowchart LR\n  C[run_multiQC] --&gt; D(all)\n  B[run_fastQC] --&gt; C[run_multiQC]\n  A[trim_fastq] --&gt; B[run_fastQC]"
  },
  {
    "objectID": "Snakemake-Essentials.html#structure-of-rules",
    "href": "Snakemake-Essentials.html#structure-of-rules",
    "title": "Snakemake Essentials",
    "section": "Structure of rules",
    "text": "Structure of rules\nRules provide crucial information to Snakemake, such as a step’s inputs, output, and the command to run. These are the bare bones of each rule, but as we develop more, we will start to also include aspects of each rule, including the Conda environment, resource requirements (time and memory), and other parameters.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    conda:\n        \"environment.yml\"\n        # or \n        \"name_of_already_installed_environment\"\n    shell:\n        \"\"\"\n        # do things to input file to make ouput file\n        \"\"\"\nInstead of shell:, users can also use run: which will run Python code.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    run:\n        # python things ..."
  },
  {
    "objectID": "Snakemake-Essentials.html#inputs-and-outputs",
    "href": "Snakemake-Essentials.html#inputs-and-outputs",
    "title": "Snakemake Essentials",
    "section": "Inputs and outputs",
    "text": "Inputs and outputs\nSnakemake traces the inputs and outputs for each rule to know what rules need to be run (and what order to run them in). These are specified very explicitly in the rule, using input: and output:, followed by an indented, comma-separated list, with one entry per line. These can also be named in the list. Another great attribute of Snakemake is that these can be referenced in the command it runs.\n\nExamples\n\n1 input, 1 output\nrule rename_file:\n    input:\n        \"old_name.txt\"\n    output:\n        \"new_name.txt\"\n    shell:\n        \"\"\"\n        mv {input} {output}\n        # same as running\n        # mv old_name.txt new_name.txt\n        \"\"\"\n\n\n2 named inputs, 2 named outputs\nrule rename_multiple_files:\n    input:\n        file_1=\"first_file.txt\",\n        file_2=\"second_file.txt\"\n    output:\n        file_1=\"file_1.txt\",\n        file_2=\"file_2.txt\"\n    shell:\n        \"\"\"\n        mv {input.file_1} {output.file_1}\n        mv {input.file_2} {output.file_2}\n        \"\"\""
  },
  {
    "objectID": "Snakemake-Essentials.html#example-with-trimming-reads",
    "href": "Snakemake-Essentials.html#example-with-trimming-reads",
    "title": "Snakemake Essentials",
    "section": "Example with trimming reads",
    "text": "Example with trimming reads\nImagine you want to trim one fastq file (10 base pairs from the beginning, 5 base pairs from the end) using SeqTK. This is what a very simple snakefile could look like:\nrule all:\n    input:\n        \"trimmed_reads.fq\"\n\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\n(We will build on this example)"
  },
  {
    "objectID": "About-Snakemake.html",
    "href": "About-Snakemake.html",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake is a workflow management tool. That’s fancy words for “it makes your life easier when you have lots of different compute jobs to run.”\nAt a very surface level, Snakemake looks for the output files you need, and if they aren’t there, it figures out all the steps that need to be run in order to get those files.\nConsider the following example. If snakemake can’t find output file, it will run process data on the input data.\n\n\n\n\nflowchart LR\n  B[process data] --&gt; C(output file)\n  A(input data) --&gt; B[process data]\n  \n\n\n\n\n\n\n\nSounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and Snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but Snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier.\n\n\n\n\n\nSnakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  },
  {
    "objectID": "About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "href": "About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "title": "About Snakemake",
    "section": "",
    "text": "Sounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and Snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but Snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier."
  },
  {
    "objectID": "About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "href": "About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  },
  {
    "objectID": "Qiime2-Applications.html",
    "href": "Qiime2-Applications.html",
    "title": "Qiime2 Applications",
    "section": "",
    "text": "Firstly, let’s make sure that you have conda, mamba-forge, and Qiime2 installed. Follow the installation instructions for the operating system of your local computer (or wherever you want to install Qiime2) which can be found here.\nOnce you have Qiime2 installed, activate your Qiime2 environment by running conda activate qiime2-2023.5 before running:\npip install snakemake\nYou now have Snakemake installed in your Qiime2 environment and can start working on your pipeline!"
  },
  {
    "objectID": "Qiime2-Applications.html#things-ive-had-to-learn-the-hard-way",
    "href": "Qiime2-Applications.html#things-ive-had-to-learn-the-hard-way",
    "title": "Qiime2 Applications",
    "section": "Things I’ve had to learn the hard way:",
    "text": "Things I’ve had to learn the hard way:\n\nAnytime you call a Qiime2 command that outputs a directory of files (like qiime diversity core-metrics-phylogenetic), Qiime and Snakemake get mad at each other because Snakemake creates the output directory for Qiime. Qiime will then give you an error about how the output directory already exists (because Snakemake created it first).\n\nInstead, you need to specify every single output file that would be in that directory in the Qiime command itself instead of just the output directory.\nrule core_metrics_analysis:\n    input:\n        \"tree.qza\",\n        \"tax_filt_actual.qza\",\n        \"metadata.tsv\"\n    output:\n        \"bray_curtis_distance_matrix.qza\",\n        \"bray_curtis_emperor.qzv\",\n        \"bray_curtis_pcoa_results.qza\",\n        \"evenness_vector.qza\",\n        \"faith_pd_vector.qza\",\n        \"jaccard_distance_matrix.qza\",\n        \"jaccard_emperor.qzv\",\n        \"jaccard_pcoa_results.qza\",\n        \"observed_features_vector.qza\",\n        \"rarefied_table.qza\",\n        \"shannon_vector.qza\",\n        \"unweighted_unifrac_distance_matrix.qza\",\n        \"unweighted_unifrac_emperor.qzv\",\n        \"unweighted_unifrac_pcoa_results.qza\",\n        \"weighted_unifrac_distance_matrix.qza\",\n        \"weighted_unifrac_emperor.qzv\",\n        \"weighted_unifrac_pcoa_results.qza\"\n    conda:\n        \"qiime2-2023.5\"\n    shell:\n        \"\"\"\n        qiime diversity core-metrics-phylogenetic \\\n            --i-phylogeny tree.qza \\\n            --i-table tax_filt_actual.qza \\\n            --p-sampling-depth  10000 \\\n            --m-metadata-file metadata.tsv \\\n            --o-rarefied-table rarefied_table.qza \\\n            --o-faith-pd-vector faith_pd_vector.qza \\\n            --o-observed-features-vector observed_features_vector.qza \\\n            --o-shannon-vector shannon_vector.qza \\\n            --o-evenness-vector evenness_vector.qza \\\n            --o-unweighted-unifrac-distance-matrix unweighted_unifrac_distance_matrix.qza \\\n            --o-weighted-unifrac-distance-matrix weighted_unifrac_distance_matrix.qza \\\n            --o-jaccard-distance-matrix jaccard_distance_matrix.qza \\\n            --o-bray-curtis-distance-matrix bray_curtis_distance_matrix.qza \\\n            --o-unweighted-unifrac-pcoa-results unweighted_unifrac_pcoa_results.qza \\\n            --o-weighted-unifrac-pcoa-results ./data/core_outputs/weighted_unifrac_pcoa_results.qza \\\n            --o-jaccard-pcoa-results ./data/core_outputs/jaccard_pcoa_results.qza \\\n            --o-bray-curtis-pcoa-results bray_curtis_pcoa_results.qza \\\n            --o-unweighted-unifrac-emperor unweighted_unifrac_emperor.qzv \\\n            --o-weighted-unifrac-emperor weighted_unifrac_emperor.qzv \\\n            --o-jaccard-emperor jaccard_emperor.qzv \\\n            --o-bray-curtis-emperor bray_curtis_emperor.qzv\n        \"\"\""
  }
]