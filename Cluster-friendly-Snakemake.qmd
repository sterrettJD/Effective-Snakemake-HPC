---
title: "Cluster-friendly Snakemake"
---

# Using a compute cluster
## Shared resource
Compute clusters are often shared resources. For example, [Fiji](https://bit.colorado.edu/biofrontiers-computing/fiji/) and [Alpine](https://www.colorado.edu/rc/alpine) are two high performance compute clusters for researchers at the University of Colorado. These resources are shared across the entire university (and sometimes multiple universities), which makes managing resources very important. 

## SLURM
Resource management for computing is often done using a job scheduler such as [Slurm](https://slurm.schedmd.com/overview.html). Slurm will be the focal job scheduling system in this resource, but others exist (the concepts remain consistent across other systems). Slurm manages the priority of compute jobs across users (based on factors such as memory/time requirements and user priority), and it manages parallelization and execution of jobs on compute nodes. 

## Batch jobs
Unlike running commands from a command line on your local computer, any compute-heavy jobs on clusters need to be done through the submission of batch jobs. Batch jobs could be considered "wrappers" for your compute jobs, with specifications of the resources needed. Slurm allocates the necessary resources on a compute node, then sends that job to be performed on the compute node. Below is an image from ETH Zurich scientific computing [wiki](https://scicomp.ethz.ch/wiki/Job_management_with_SLURM) demonstrating this process.
![Figure of batch job submission](figures/1000px-Batch_system.png)

## Example batch jobs submission
This often involved running a `.sbatch` script, which has the resource requirements at the top. Here is an example for trimming reads in a fastq file:
```{snakemake}
#SBATCH --mail-user=<email_address>@colorado.edu
#SBATCH --partition=short # run on the short partition
#SBATCH --nodes=1 # Only use a single node
#SBATCH --ntasks=1 # Run with one thread
#SBATCH --mem=8gb # Memory limit
#SBATCH --time=02:00:00 # Time limit hrs:min:sec
#SBATCH --output=/path/to/here/logs/trim_reads_%j.out # Standard output and error log
#SBATCH --error=/path/to/here/logs/trim_reads_%j.err # %j inserts job number

# activate conda environment
source activate seqtk_env

# trim reads
seqtk trimfq -b 10 -e 5 raw_reads.fq > trimmed_reads.fq
```

Which could be run with the following command:
```
sbatch trim_reads.sbatch
```


# Setting up a SLURM profile
This section will explain how to set up a SLURM profile to integrate Snakemake with the job scheduling system, SLURM.

## Why it matters
SLURM profiles are advantageous because they manage submission/monitoring of each step in your Snakemake pipeline as separate batch jobs. For example, if you have 50 samples to process through 2 rules, the SLURM profile will submit/monitor each rule & sample combination as a separate job, so: 

  1. These jobs can run in parallel, with unique resource requirements for each job.
  2. It is easy to monitor the job status for each unique sample/step.
  3. If one fails, the others will keep going. 

Without a SLURM profile, Snakemake would try to run each job sequentially within the batch job that you've submitted to run Snakemake, and this could get cumbersome. Additionally, imagine step 1 requires 2 GB of memory, but step 2 requires 100 GB memory. Without submitting individual batch jobs for each step/sample, you would have to request 100 GB memory for the full duration, which would result in a waste of resources on the shared cluster while the first step is running. 

## Using cookiecutter
### About the profile
I recommend using the [official Snakemake SLURM profile](https://github.com/Snakemake-Profiles/slurm), which you can install using [cookiecutter](https://cookiecutter.readthedocs.io/en/stable/README.html). This profile manages job submission and monitors submitted jobs. 

If you plan on running a Snakemake pipeline with **many many** steps that do not take very long to run (i.e., you're submitting tens of thousands of jobs that run quickly, such that the submission is the bottleneck), you may want to check out John Blischak's [simple Snakemake SLURM profile](https://github.com/jdblischak/smk-simple-slurm), which is does not have as many status checks and submits jobs much faster. If job submission isn't a bottleneck, it's best to use the official Snakemake SLURM profile ([use speed with caution](https://github.com/jdblischak/smk-simple-slurm#use-speed-with-caution)). 

### Installating cookiecutter
In your Snakemake Conda environment:
```
pip install pipx
pipx install cookiecutter
```

### Snakemake SLURM profile setup
In your Snakemake Conda environment:
```
# create config directory that snakemake searches for profiles (or use something else)
profile_dir="${HOME}/.config/snakemake"
mkdir -p "$profile_dir"
# use cookiecutter to create the profile in the config directory
template="gh:Snakemake-Profiles/slurm"
cookiecutter --output-dir "$profile_dir" "$template"
```

Then, hit enter to follow all of the default prompts, **except** make sure `use_conda` is `True`. That will look like this: 
![screenshot of profile setup](figures/profile-setup.png)


# Managing resources
