[
  {
    "objectID": "quarto/Elaborating-rules.html",
    "href": "quarto/Elaborating-rules.html",
    "title": "Elaborating rules",
    "section": "",
    "text": "We’ve covered how to provide inputs and outputs for a rule, but we can actually provide anything we want as parameters of a rule. These rule params can be accessed with curly brackets just like we access inputs and outputs. For example:\nrule count_ducks:\n    output: \n        \"ducks_file.txt\"\n    params:\n        ducks=2,\n        eggs=4\n    shell:\n        \"\"\"\n        echo \"The {params.ducks} ducks laid {params.eggs} eggs!\" &gt; {output}\n        \"\"\"\nThis would print “The 2 ducks laid 4 eggs!” to the output file, filling the params into the shell command, like it does with {output}."
  },
  {
    "objectID": "quarto/Elaborating-rules.html#why-would-i-ever-do-it-this-way",
    "href": "quarto/Elaborating-rules.html#why-would-i-ever-do-it-this-way",
    "title": "Elaborating rules",
    "section": "Why would I ever do it this way?",
    "text": "Why would I ever do it this way?\nYou may be wondering, “can’t I just hard code this in the shell command?” You absolutely could. It would even save you a few lines of code in these examples.\nHowever, providing params and threads as components of the rule provides two main benefits:\n\nIt allows us to also start drawing these variables from config files, which we’ll get into on the next page.\nIt increases readability. You no longer need to sift through your code to figure out which exact parameter you provided; this way, you can look at the rule (or ideally your config file), and it should jump out at you."
  },
  {
    "objectID": "quarto/Elaborating-rules.html#using-environment-name-or-path-simplest-but-not-recommended",
    "href": "quarto/Elaborating-rules.html#using-environment-name-or-path-simplest-but-not-recommended",
    "title": "Elaborating rules",
    "section": "Using environment name or path (simplest but not recommended)",
    "text": "Using environment name or path (simplest but not recommended)\nIf you have already built the conda environment that you want to use, you can provide either the name or filepath for that directory. Sometimes, filepaths are generally more robust than names, but neither are particularly portable across machines.\nThe below output from conda env list shows the possibility of four environments that could me passed to snakemake.\nconda env list\n# conda environments:\n#\nbase                     /Users/&lt;username&gt;/mambaforge\nHoMi                     /Users/&lt;username&gt;/mambaforge/envs/HoMi\n                         /Users/&lt;username&gt;/anaconda3/envs/Unnamed_env\nFrom these environments, we could pass any of the following to a rule:\n\nconda: \"base\" OR conda: \"/Users/&lt;username&gt;/mambaforge\"\nconda: \"HoMi\" OR conda: \"/Users/&lt;username&gt;/mambaforge/envs/HoMi\"\nconda: \"/Users/&lt;username&gt;/anaconda3/envs/Unnamed_env\". There is no name version of this environment.\n\nFor the HoMi environment, this could look like such:\n\nrule run_HoMi:\n   output:\n       \"HoMi.out\"\n   conda: \"HoMi\" # or conda: \"/Users/&lt;username&gt;/mambaforge/envs/HoMi\"\n   shell:\n       \"\"\"\n       HoMi.py -o {output}\n       \"\"\"\nNot all environments can be accessed by name, which can occur if conda settings have changed. For example, changing from using anaconda to mambaforge could cause conda to stop recognizing names for the anaconda environments. Due to this, I recommend passing conda environment paths instead of names. However, the YAML approach is more robust and portable, so I’d recommend that over either names or filepaths."
  },
  {
    "objectID": "quarto/Elaborating-rules.html#using-conda-environment-yaml-recommended",
    "href": "quarto/Elaborating-rules.html#using-conda-environment-yaml-recommended",
    "title": "Elaborating rules",
    "section": "Using conda environment YAML (recommended)",
    "text": "Using conda environment YAML (recommended)\nHere is an example of the conda environment yaml syntax:\nrule run_fastqc:\n  input:\n    \"{sample}.trimmed.{read}.fq\"\n  output:\n    \"{sample}.trimmed.{read}_fastqc.zip\"\n  conda: \"conda_envs/fastqc.yaml\"\n  shell:\n    \"\"\"\n    fastqc {input} -o .\n    \"\"\"\nIn this case, conda_envs/fastqc.yaml should be a .yaml formatted file with the conda environment’s dependencies. Here is an example:\nname: fastqc_environment\nchannels:\n  - biobakery\n  - conda-forge\n  - bioconda\n  - defaults\ndependencies:\n  - multiqc\n  - fastqc\nWhen provided an environment YAML, Snakemake will create it in the directory .snakemake/conda/$hash, where $hash is a hash that Snakemake has given the conda environment. I recommend modularizing your conda environments as much as possible. Having unique conda environments for distinct portions of your workflow can avoid dependency issues.\n\nCreating YAMLs from existing environments\nYou can easily create .yaml files for existing environments by using the command conda env export -n &lt;env_name&gt; --from-history &gt; &lt;env_name&gt;.yaml, then by manually adding any packages installed via pip. Without the --from-history flag, conda will export the exact package version hashes, which will hinder portability of the environment across different computers, as these versions are operating system specific. You can also add some version numbers to packages in these .yaml files, such fastqc=0.12.1. More information can be found here."
  },
  {
    "objectID": "quarto/Elaborating-rules.html#combining-use-of-conda-environment-name-and-yaml-files",
    "href": "quarto/Elaborating-rules.html#combining-use-of-conda-environment-name-and-yaml-files",
    "title": "Elaborating rules",
    "section": "Combining use of conda environment name and YAML files",
    "text": "Combining use of conda environment name and YAML files\nYou can use a combination of the two above techniques if your environments take a long time to install or have specific installation requirements that can’t be put in the .yaml file. You can create .yaml files for your needed conda environments and then use those to create the environments prior to running your workflow. I like to put mine in a bash script with any needed installation requirements. For example:\n#!/bin/bash\n\nset -e\nset -u\nset -o\n\n# qiime2 has specific installation requirements and will give you issues if you simply try to use the .yaml file\necho \"--------creating qiime environment\"\n# make sure that the qiime installation that you have in here is for the proper software system\n# these instructions are for apple silicon (M1 and M2 chips)\nwget https://data.qiime2.org/distro/core/qiime2-2023.5-py38-osx-conda.yml\nCONDA_SUBDIR=osx-64 conda env create -n qiime2-2023.5 --file qiime2-2023.5-py38-osx-conda.yml\nconda config --env --set subdir osx-64\nrm qiime2-2023.5-py38-osx-conda.yml\nconda activate qiime2-2023.5\npip install snakemake\nconda deactivate\n\necho \"--------creating R envrionment\"\nconda env create -f r_env.yml\n\necho \"--------creating picrust environment\"\nconda env create -f picrust2.yml\nAnd then run the bash script via:\nsh my_environments.sh\nFrom there, you can reference the conda environment name or path in your Snakefile. Since these .yaml files can be used to create environments on any machine, this method adds portability and reproducibility while breaking up the process computationally and allows you to get around issues with specific installation requirements. However, if your conda environment .yaml files work just fine, I would reccommend sticking with that method."
  },
  {
    "objectID": "quarto/Elaborating-rules.html#limiting-resources",
    "href": "quarto/Elaborating-rules.html#limiting-resources",
    "title": "Elaborating rules",
    "section": "Limiting resources",
    "text": "Limiting resources\nWhen running Snakemake locally, specifying resources can be useful for making sure Snakemake doesn’t overrun the resources available on your computer. On the command line, a flag for maximum resources used can be passed. For example, if you wanted to limit Snakemake to 8 GB total memory, you could run it using the command snakemake --cores 1 --resources mem_mb=8000. If you needed to run the run_fastq rule above for 50 samples but passed 8 GB as the max memory footprint, Snakemake would make sure no more than 4 samples were running at a time."
  },
  {
    "objectID": "quarto/Elaborating-rules.html#snakemake-doesnt-monitor-resource-usage",
    "href": "quarto/Elaborating-rules.html#snakemake-doesnt-monitor-resource-usage",
    "title": "Elaborating rules",
    "section": "Snakemake doesn’t monitor resource usage",
    "text": "Snakemake doesn’t monitor resource usage\nSnakemake does not monitor real-time resource useage but instead just goes off of the benchmarks you provide it. If you say a rule needs 2 GB memory, but it actually uses 10 GB, Snakemake will not know or adjust how it is scheduling the rules. More information on resources can be found here."
  },
  {
    "objectID": "quarto/Elaborating-rules.html#resources-are-most-useful-for-cluster-integration",
    "href": "quarto/Elaborating-rules.html#resources-are-most-useful-for-cluster-integration",
    "title": "Elaborating rules",
    "section": "Resources are most useful for cluster integration",
    "text": "Resources are most useful for cluster integration\nResources specifications become very useful when integrating Snakemake with a compute cluster with some sort of resource management system like Slurm. This is discussed in Cluster-friendly Snakemake, but a Slurm profile for Snakemake can look at the resource requirements for each rule and request that many resources when submitting batch job requests on the cluster."
  },
  {
    "objectID": "quarto/Qiime2-Applications.html",
    "href": "quarto/Qiime2-Applications.html",
    "title": "Qiime2 Applications",
    "section": "",
    "text": "Firstly, let’s make sure that you have conda, mamba-forge, and Qiime2 installed. Follow the installation instructions for the operating system of your local computer (or wherever you want to install Qiime2) which can be found here.\nOnce you have Qiime2 installed, activate your Qiime2 environment by running conda activate qiime2-2023.5 before running:\npip install snakemake\nYou now have Snakemake installed in your Qiime2 environment and can start working on your pipeline!"
  },
  {
    "objectID": "quarto/Qiime2-Applications.html#things-ive-had-to-learn-the-hard-way",
    "href": "quarto/Qiime2-Applications.html#things-ive-had-to-learn-the-hard-way",
    "title": "Qiime2 Applications",
    "section": "Things I’ve had to learn the hard way:",
    "text": "Things I’ve had to learn the hard way:\n\nAnytime you call a Qiime2 command that outputs a directory of files (like qiime diversity core-metrics-phylogenetic), Qiime and Snakemake get mad at each other because Snakemake creates the output directory before Qiime. Qiime will then give you an error about how the output directory already exists (because Snakemake created it first).\n\nInstead, you need to specify that the output you’re generating in your shell is an entire directory by using output = directory(\"directory_name\"). For example:\nrule all:\n    input:\n        # can reference output directory like so here\n        \"core_metrics_directory\"\n\n\nrule core_metrics_analysis:\n    input:\n        TREE=\"tree.qza\",\n        TABLE=\"taxonomy_filtered.qza\",\n        METADATA=\"metadata.tsv\"\n    output:\n        OUTPUT_DIR=directory(\"core_metrics_directory\")\n    params:\n        sampling_depth=10000\n    shell:\n        \"\"\"\n        qiime diversity core-metrics-phylogenetic \\\n            --i-phylogeny {input.TREE} \\\n            --i-table {input.TABLE} \\\n            --p-sampling-depth {params.sampling_depth} \\\n            --m-metadata-file {input.METADATA} \\\n            --output-dir {output} \n        \"\"\"\n\nI used to list every single output file that would be in that directory in the Qiime command itself instead of just the output directory. This also works but it get’s a bit annoying after a while, especially if the output directory being generated has file names that change with each run. For example:\nrule core_metrics_analysis:\n    input:\n        TREE=\"tree.qza\",\n        TABLE=\"tax_filt_actual.qza\",\n        METADATA=\"metadata.tsv\"\n    output:\n        BCDM=\"bray_curtis_distance_matrix.qza\",\n        BCEMP=\"bray_curtis_emperor.qzv\",\n        BCPCOA=\"bray_curtis_pcoa_results.qza\",\n        EVEN=\"evenness_vector.qza\",\n        FAITH=\"faith_pd_vector.qza\",\n        JDM=\"jaccard_distance_matrix.qza\",\n        JEMP=\"jaccard_emperor.qzv\",\n        JPCOA=\"jaccard_pcoa_results.qza\",\n        OF=\"observed_features_vector.qza\",\n        RAREFIED=\"rarefied_table.qza\",\n        SHANNON=\"shannon_vector.qza\",\n        UUDM=\"unweighted_unifrac_distance_matrix.qza\",\n        UUEMP=\"unweighted_unifrac_emperor.qzv\",\n        UUPCOA=\"unweighted_unifrac_pcoa_results.qza\",\n        WUDM=\"weighted_unifrac_distance_matrix.qza\",\n        WUEMP=\"weighted_unifrac_emperor.qzv\",\n        WUPCOA=\"weighted_unifrac_pcoa_results.qza\"\n    conda:\n        \"qiime2-2023.5\"\n    params:\n        sampling_depth=10000\n    shell:\n        \"\"\"\n        qiime diversity core-metrics-phylogenetic \\\n            --i-phylogeny {input.TREE} \\\n            --i-table {input.TABLE} \\\n            --p-sampling-depth {params.sampling_depth} \\\n            --m-metadata-file {input.METADATA} \\\n            --o-rarefied-table {output.RAREFIED} \\\n            --o-faith-pd-vector {output.FAITH} \\\n            --o-observed-features-vector {output.OF} \\\n            --o-shannon-vector {output.SHANNON} \\\n            --o-evenness-vector {output.EVEN} \\\n            --o-unweighted-unifrac-distance-matrix {output.UUDM} \\\n            --o-weighted-unifrac-distance-matrix {output.WUDM} \\\n            --o-jaccard-distance-matrix {output.JDM} \\\n            --o-bray-curtis-distance-matrix {output.BCDM} \\\n            --o-unweighted-unifrac-pcoa-results {output.UUPCOA} \\\n            --o-weighted-unifrac-pcoa-results {output.WUPCOA} \\\n            --o-jaccard-pcoa-results {output.JPCOA} \\\n            --o-bray-curtis-pcoa-results {output.BCPCOA} \\\n            --o-unweighted-unifrac-emperor {output.UUEMP} \\\n            --o-weighted-unifrac-emperor {output.WUEMP} \\\n            --o-jaccard-emperor {output.JEMP} \\\n            --o-bray-curtis-emperor {output.BCEMP}\n        \"\"\"\n\nSince many of the steps in a Qiime2 workflow are interactive, put each step in it’s own Snakemake rule.\n\nI originally thought that it would be more streamlined to put all of the Qiime2 workflow code into a regular bash script and to reference that in my Snakemake rules (it would certainly make my snakefile shorter). However, since many of the outputs in the Qiime2 workflow need to be referenced to inform the parameters on downstream steps, leaving each step in it’s own rule allows you to easily check generated outputs and adjust subsequent parameters. This is also particularly helpful when you need to re-run your workflow (as that inevitably happens) since Snakemake will only re-run rules that have been altered or are missing outputs so the super time-consuming Qiime2 steps will only be run once. If you had everything in a bash script, it would be re-run with your workflow every time, which is a lot of time and computational power that will slow down your analysis."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html",
    "href": "quarto/Cluster-friendly-Snakemake.html",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Compute clusters are often shared resources. For example, Fiji and Alpine are two high performance compute clusters for researchers at the University of Colorado. These resources are shared across the entire university (and sometimes multiple universities), which makes managing resources very important.\n\n\n\nResource management for computing is often done using a job scheduler such as Slurm. Slurm will be the focal job scheduling system in this resource, but others exist (the concepts remain consistent across other systems). Slurm manages the priority of compute jobs across users (based on factors such as memory/time requirements and user priority), and it manages parallelization and execution of jobs on compute nodes.\n\n\n\nUnlike running commands from a command line on your local computer, any compute-heavy jobs on clusters need to be done through the submission of batch jobs. Batch jobs could be considered “wrappers” for your compute jobs, with specifications of the resources needed. Slurm allocates the necessary resources on a compute node, then sends that job to be performed on the compute node. Below is an image from ETH Zurich scientific computing wiki demonstrating this process. \n\n\n\nThis often involved running a .sbatch script, which has the resource requirements at the top. Here is an example for trimming reads in a fastq file:\n#SBATCH --mail-user=&lt;email_address&gt;@colorado.edu\n#SBATCH --partition=short # run on the short partition\n#SBATCH --nodes=1 # Only use a single node\n#SBATCH --ntasks=1 # Run with one thread\n#SBATCH --mem=8gb # Memory limit\n#SBATCH --time=02:00:00 # Time limit hrs:min:sec\n#SBATCH --output=/path/to/here/logs/trim_reads_%j.out # Standard output and error log\n#SBATCH --error=/path/to/here/logs/trim_reads_%j.err # %j inserts job number\n\n# activate conda environment\nsource activate seqtk_env\n\n# trim reads\nseqtk trimfq -b 10 -e 5 raw_reads.fq &gt; trimmed_reads.fq\nWhich could be run with the following command:\nsbatch trim_reads.sbatch\n\n\n\nThis works well for single jobs, but again, when things start to scale up (50 samples, 10 steps per sample, etc..), this can become cumbersome.\n\n\nWhen I was new to working on clusters, I found myself writing python scripts to loop through each of my samples, find their files, then run sbatch scripts that had to parse/sanitize multiple parameters. I certainly could have done this more simply, but it quickly snowballed into a 50-150 line python script with a &gt;25 line sbatch script per step, leading to a &gt;&gt;1000 line codebase for a project with just a few processing steps. The last straw was when I needed to go back and run step 1 with new parameters, meaning I had to manually rerun every following step (after waiting for all samples to finish each step).\nUsing Snakemake, to manage I was able to reduce this codebase by ~70% and reduce my manual input by ~50%. To quote Casey Martin, “Compute flops are cheap and nearly unlimited at our level; human flops are expensive and limited,” so we need to prioritize efficiency in our human flops, using tools like Snakemake.\n\n\n\nMy favorite part of using Snakemake has been that once a Slurm profile is set up, it will handle all of the sbatch job submission for you. This “profile” is really just a little tool that creates the .sbatch script, submits it, logs output/error, and monitors completion. In your Snakefile, you can specify resource requirements and conda environment, such that the previous script would look like this:\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"short\",\n        mem_mb=8000,\n        runtime=120 #min\n    threads: 1\n    conda: \"seqtk_env\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\nIn this case it doesn’t look much more concise, but it is much easier to generalize this rule to run multiple samples/parameters, and to chain multiple samples together. Most importantly, it’s easier to read and debug!\n\n“Programs must be written for people to read, and only incidentally for machines to execute.” ― Harold Abelson, Structure and Interpretation of Computer Programs"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#shared-resource",
    "href": "quarto/Cluster-friendly-Snakemake.html#shared-resource",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Compute clusters are often shared resources. For example, Fiji and Alpine are two high performance compute clusters for researchers at the University of Colorado. These resources are shared across the entire university (and sometimes multiple universities), which makes managing resources very important."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#slurm",
    "href": "quarto/Cluster-friendly-Snakemake.html#slurm",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Resource management for computing is often done using a job scheduler such as Slurm. Slurm will be the focal job scheduling system in this resource, but others exist (the concepts remain consistent across other systems). Slurm manages the priority of compute jobs across users (based on factors such as memory/time requirements and user priority), and it manages parallelization and execution of jobs on compute nodes."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#batch-jobs",
    "href": "quarto/Cluster-friendly-Snakemake.html#batch-jobs",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Unlike running commands from a command line on your local computer, any compute-heavy jobs on clusters need to be done through the submission of batch jobs. Batch jobs could be considered “wrappers” for your compute jobs, with specifications of the resources needed. Slurm allocates the necessary resources on a compute node, then sends that job to be performed on the compute node. Below is an image from ETH Zurich scientific computing wiki demonstrating this process."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#example-batch-jobs-submission",
    "href": "quarto/Cluster-friendly-Snakemake.html#example-batch-jobs-submission",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "This often involved running a .sbatch script, which has the resource requirements at the top. Here is an example for trimming reads in a fastq file:\n#SBATCH --mail-user=&lt;email_address&gt;@colorado.edu\n#SBATCH --partition=short # run on the short partition\n#SBATCH --nodes=1 # Only use a single node\n#SBATCH --ntasks=1 # Run with one thread\n#SBATCH --mem=8gb # Memory limit\n#SBATCH --time=02:00:00 # Time limit hrs:min:sec\n#SBATCH --output=/path/to/here/logs/trim_reads_%j.out # Standard output and error log\n#SBATCH --error=/path/to/here/logs/trim_reads_%j.err # %j inserts job number\n\n# activate conda environment\nsource activate seqtk_env\n\n# trim reads\nseqtk trimfq -b 10 -e 5 raw_reads.fq &gt; trimmed_reads.fq\nWhich could be run with the following command:\nsbatch trim_reads.sbatch"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#using-snakemake-to-automate-batch-job-submission",
    "href": "quarto/Cluster-friendly-Snakemake.html#using-snakemake-to-automate-batch-job-submission",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "This works well for single jobs, but again, when things start to scale up (50 samples, 10 steps per sample, etc..), this can become cumbersome.\n\n\nWhen I was new to working on clusters, I found myself writing python scripts to loop through each of my samples, find their files, then run sbatch scripts that had to parse/sanitize multiple parameters. I certainly could have done this more simply, but it quickly snowballed into a 50-150 line python script with a &gt;25 line sbatch script per step, leading to a &gt;&gt;1000 line codebase for a project with just a few processing steps. The last straw was when I needed to go back and run step 1 with new parameters, meaning I had to manually rerun every following step (after waiting for all samples to finish each step).\nUsing Snakemake, to manage I was able to reduce this codebase by ~70% and reduce my manual input by ~50%. To quote Casey Martin, “Compute flops are cheap and nearly unlimited at our level; human flops are expensive and limited,” so we need to prioritize efficiency in our human flops, using tools like Snakemake.\n\n\n\nMy favorite part of using Snakemake has been that once a Slurm profile is set up, it will handle all of the sbatch job submission for you. This “profile” is really just a little tool that creates the .sbatch script, submits it, logs output/error, and monitors completion. In your Snakefile, you can specify resource requirements and conda environment, such that the previous script would look like this:\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"short\",\n        mem_mb=8000,\n        runtime=120 #min\n    threads: 1\n    conda: \"seqtk_env\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\nIn this case it doesn’t look much more concise, but it is much easier to generalize this rule to run multiple samples/parameters, and to chain multiple samples together. Most importantly, it’s easier to read and debug!\n\n“Programs must be written for people to read, and only incidentally for machines to execute.” ― Harold Abelson, Structure and Interpretation of Computer Programs"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#why-it-matters",
    "href": "quarto/Cluster-friendly-Snakemake.html#why-it-matters",
    "title": "Cluster-friendly Snakemake",
    "section": "Why it matters",
    "text": "Why it matters\nSlurm profiles are advantageous because they manage submission/monitoring of each step in your Snakemake pipeline as separate batch jobs. For example, if you have 50 samples to process through 10 rules, the SLURM profile will submit/monitor each rule & sample combination as a separate job, so:\n\nThese jobs can run in parallel, with unique resource requirements for each job.\nIt is easy to monitor the job status for each unique sample/step.\nIf one fails, the others will keep going.\n\nWithout a Slurm profile, Snakemake would try to run each job sequentially within the batch job that you’ve submitted to run Snakemake, and this could get cumbersome. Additionally, imagine step 1 requires 2 GB of memory, but step 2 requires 100 GB memory. Without submitting individual batch jobs for each step/sample, you would have to request 100 GB memory for the full duration, which would result in a waste of resources on the shared cluster while the first step is running."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#using-cookiecutter",
    "href": "quarto/Cluster-friendly-Snakemake.html#using-cookiecutter",
    "title": "Cluster-friendly Snakemake",
    "section": "Using cookiecutter",
    "text": "Using cookiecutter\n\nAbout the profile\nI recommend using the official Snakemake Slurm profile, which you can install using cookiecutter. This profile manages job submission and monitors submitted jobs.\nIf you plan on running a Snakemake pipeline with many many steps that do not take very long to run (i.e., you’re submitting tens of thousands of jobs that run quickly, such that the submission is the bottleneck), you may want to check out John Blischak’s simple Snakemake Slurm profile, which is does not have as many status checks and submits jobs much faster. If job submission isn’t a bottleneck, it’s best to use the official Snakemake SLURM profile (use speed with caution).\n\n\nInstallating cookiecutter\nIn your Snakemake Conda environment:\npip install pipx\npipx install cookiecutter\n\n\nSnakemake SLURM profile setup\nIn your Snakemake Conda environment:\n# create config directory that snakemake searches for profiles (or use something else)\nprofile_dir=\"${HOME}/.config/snakemake\"\nmkdir -p \"$profile_dir\"\n# use cookiecutter to create the profile in the config directory\ntemplate=\"gh:Snakemake-Profiles/slurm\"\ncookiecutter --output-dir \"$profile_dir\" \"$template\"\nThen, hit enter to follow all of the default prompts, except make sure use_conda is True. That will look like this:"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#tell-snakemake-about-your-profile",
    "href": "quarto/Cluster-friendly-Snakemake.html#tell-snakemake-about-your-profile",
    "title": "Cluster-friendly Snakemake",
    "section": "Tell Snakemake about your profile",
    "text": "Tell Snakemake about your profile\nWhen running Snakemake with a profile, you need to use the --profile flag, like this:\nsnakemake --profile ~/.config/snakemake/slurm"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#providing-resources-in-rules",
    "href": "quarto/Cluster-friendly-Snakemake.html#providing-resources-in-rules",
    "title": "Cluster-friendly Snakemake",
    "section": "Providing resources in rules",
    "text": "Providing resources in rules\nIn your rule, you can pass resources much like passing input/outputs specs. Threads are generally passed outside of the resources section. Additionally, any specifications for a rule can be accessed in the rule’s shell command, using syntax such as {resources.partition} to access the partition name, or {threads} to access the number of threads. Typically, you shouldn’t need to access Slurm-specific aspects like the partition (your profile is handling this), but for some command line tools, where you can specifiy resources, it’s useful to pass {threads} into your shell command.\nYou can consult the profile’s documentation for all of the options, but this is how I pass my resources.\nrule parallelizable_job:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"&lt;partition_name&gt;\",\n        mem_mb=int(8*1000), # 8 GB\n        runtime=int(2*60), # min, or 2 hours\n        slurm=\"mail-user=&lt;email_address&gt;@colorado.edu\"\n    threads: 8\n    shell:\n        \"\"\"\n        run_parallelizable_task --threads {threads} --max_memory {resources.mem_mb}\n        \"\"\"\nAdditionally, you can generally pass any “long format” names for Slurm parameters in your resources.slurm string. More details on that formatting can be found here."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#rules-from-a-config-file",
    "href": "quarto/Cluster-friendly-Snakemake.html#rules-from-a-config-file",
    "title": "Cluster-friendly Snakemake",
    "section": "Rules from a config file",
    "text": "Rules from a config file\n\nPassing a config file to Snakemake\nIt’s recommended to provide rule-specific resources in a config file. Config files for Snakemake are .yaml format, and they should be specified when running Snakemake from the command line, as such:\nsnakemake --profile ~/.config/snakemake/slurm --config Snakemake_config.yaml\n\n\nUsing a config for resources\nOnce a config file has been provided to Snakemake, it will recognize the variable config in your snakefile, which is a Python dictionary.\nIf your config file looks like this:\nparallelizable_job_threads: 8\nparallelizable_job_mem_mb: 8000\nYour Snakemake rule can be written as such:\nrule parallelizable_job:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"&lt;partition_name&gt;\",\n        mem_mb=config.get(\"parallelizable_job_mem_mb\"), # 8 GB\n        runtime=int(2*60), # min, or 2 hours\n        slurm=\"mail-user=&lt;email_address&gt;@colorado.edu\"\n    threads: config.get(\"parallelizable_job_threads\")\n    shell:\n        \"\"\"\n        run_parallelizable_task --threads {threads} --max_memory {resources.mem_mb}\n        \"\"\"\n\n\nSanitizing config resources\nI personally like to set default resources for each rule in my Snakefile and sanitize the config resources. I’m not sure if it’s “best practices”, but it feels safer. For example, if I accidentally delete the memory param from my config, I don’t want the Snakemake to instead pass the default memory param to Slurm.\n\nStory time - don’t forget the resource requirements!!\nI once left out some important resource requirements, leading to crashing two nodes of a compute cluster. When reconstructing genomes from metagenomes (MAGs) using MetaSPAdes (which needed ~150GB per sample), I accidentally deleted the memory requirement in my config file, and Snakemake submitted 50 jobs to Slurm, requesting the default of 8GB per sample. Slurm scheduled all of these jobs on two compute nodes with 500GB total RAM, thinking I needed ~400GB total, when I really needed ~7500GB, and I crashed these compute nodes and received a scary email from the cluster admins. Learn from my mistakes!\n\n\nHow I avoid issues\nI write functions such as this one to handle the resources I’m passing to each rule:\n\ndef get_threads(rule_default, config, rule_name):\n    config_param = config.get(f\"{rule_name}_threads\")\n    if config_param is not None:\n        return int(config_param)\n\n    return rule_default\n\nAnd my rule would look like this:\nrule my_rule:\n    output:\n        \"out_file.txt\"\n    threads: get_threads(rule_default=1, config, \"my_rule\")\n    shell:\n        \"\"\"\n        do things...\n        \"\"\"\nThis function looks for rule-specific threads in the config, formatted as “_threads”. If that exists, it will use the rule-specific threads from the config, but if not, we already have a default rule-specific number of threads specified. I like this structure because it keeps my config file concise while still maintaining the ability to alter rules’ resources.\nIf you have a better way to do this, feel free to open an issue/pull request, and I can add it!"
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html",
    "href": "quarto/Snakemake-Essentials.html",
    "title": "Snakemake Essentials",
    "section": "",
    "text": "Installation instructions can be found on the Snakemake documentation page here. In short, you want to install conda and mamba-forge, then run:\nmamba create -c conda-forge -c bioconda -n snakemake_env snakemake\nAlternatively, if you want to install Snakemake in an environment you already have set up, first activate that environment, then run:\npip install snakemake\nThis is especially important if you will be using more than one conda environment in your pipeline."
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#what-is-rule-all",
    "href": "quarto/Snakemake-Essentials.html#what-is-rule-all",
    "title": "Snakemake Essentials",
    "section": "What is “rule all”?",
    "text": "What is “rule all”?\nAt this point, you may be asking “Wait! You said I should have a rule for each step! What’s this rule all mess??”\nrule all is how we specify the output files we want, and it’s located at the very top of your snakefile. You can specify target files (pipeline endpoints) using the input to rule all. In a scientific case, consider these files to be the input to the paper you’ll write from your analysis.\nSnakemake is tracking what rules need to be run in order to generate the inputs for other rules, so it will track that - rule run_multiQC’s outputs -&gt; rule all’s inputs - rule run_fastQC’s outputs -&gt; rule run_multiQC’s inputs - rule trim_fastq’s outputs -&gt; rule run_fastQC’s inputs\nTherefore, it knows that in order to have the final files, it will use rule trim_fastq -&gt; rule run_fastQC -&gt; rule run_multiQC -&gt; rule all\n\n\n\n\nflowchart LR\n  C[run_multiQC] --&gt; D(all)\n  B[run_fastQC] --&gt; C[run_multiQC]\n  A[trim_fastq] --&gt; B[run_fastQC]"
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#structure-of-rules",
    "href": "quarto/Snakemake-Essentials.html#structure-of-rules",
    "title": "Snakemake Essentials",
    "section": "Structure of rules",
    "text": "Structure of rules\nRules provide crucial information to Snakemake, such as a step’s inputs, output, and the command to run. These are the bare bones of each rule, but as we develop more, we will start to also include aspects of each rule, including the Conda environment, resource requirements (time and memory), and other parameters.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    shell:\n        \"\"\"\n        # do things to input file to make ouput file\n        \"\"\"\nInstead of shell:, users can also use run: which will run Python code.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    run:\n        # python things ..."
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#inputs-and-outputs",
    "href": "quarto/Snakemake-Essentials.html#inputs-and-outputs",
    "title": "Snakemake Essentials",
    "section": "Inputs and outputs",
    "text": "Inputs and outputs\nSnakemake traces the inputs and outputs for each rule to know what rules need to be run (and what order to run them in). These are specified very explicitly in the rule, using input: and output:, followed by an indented, comma-separated list, with one entry per line. These can also be named in the list. Another great attribute of Snakemake is that these can be referenced in the command it runs.\n\nExamples\n\n1 input, 1 output\nrule rename_file:\n    input:\n        \"old_name.txt\"\n    output:\n        \"new_name.txt\"\n    shell:\n        \"\"\"\n        mv {input} {output}\n        # same as running\n        # mv old_name.txt new_name.txt\n        \"\"\"\n\n\n2 named inputs, 2 named outputs\nrule rename_multiple_files:\n    input:\n        file_1=\"first_file.txt\",\n        file_2=\"second_file.txt\"\n    output:\n        file_1=\"file_1.txt\",\n        file_2=\"file_2.txt\"\n    shell:\n        \"\"\"\n        mv {input.file_1} {output.file_1}\n        mv {input.file_2} {output.file_2}\n        \"\"\""
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#example-with-trimming-reads",
    "href": "quarto/Snakemake-Essentials.html#example-with-trimming-reads",
    "title": "Snakemake Essentials",
    "section": "Example with trimming reads",
    "text": "Example with trimming reads\n\nSimple\nImagine you want to trim one fastq file (10 base pairs from the beginning, 5 base pairs from the end) using SeqTK. This is what a very simple snakefile could look like:\nrule all:\n    input:\n        \"trimmed_reads.fq\"\n\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\n\n\nChaining rules\nNow, imagine the raw reads are compressed. We want to unzip them, trim the reads, and recompress them. You could do this in 1 step, but let’s break it up for the sake of learning. That would look like this:\nrule all:\n    input:\n        \"trimmed_reads.fq.gz\"\n\nrule unzip_fastq:\n    input:\n        \"raw_reads.fq.gz\"\n    output:\n        \"raw_reads.fq\"\n    shell:\n        \"\"\"\n        gunzip {input}\n        \"\"\"\n\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\n\nrule zip_trimmed:\n    input:\n        \"trimmed_reads.fq\"\n    output:\n        \"trimmed_reads.fq.gz\"\n    shell:\n        \"\"\"\n        gzip {input}\n        \"\"\"\nWhich would create a workflow like this:\n\n\n\n\nflowchart LR\n  C[zip_trimmed] --&gt; D(all)\n  B[trim_fastq] --&gt; C[zip_trimmed]\n  A[unzip_fastq] --&gt; B[trim_fastq]\n\n\n\n\n\n(We will build on this example)"
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#about-wildcards",
    "href": "quarto/Snakemake-Essentials.html#about-wildcards",
    "title": "Snakemake Essentials",
    "section": "About wildcards",
    "text": "About wildcards\nWildcards are a big part of how we can expand and generalize how our snakemake pipeline works. Consider a wildcards to be a list of values for which you want to run a snakemake rule multiple times. This is a bit like a for loop:\n\nfor value in wildcards: \n    run rule..."
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#syntax",
    "href": "quarto/Snakemake-Essentials.html#syntax",
    "title": "Snakemake Essentials",
    "section": "Syntax",
    "text": "Syntax\n\nOne wildcard\nWe can expand a rule using wildcards by expand()ing the input to a rule that requires this rule’s output. For example:\nrule all:\n    input:\n        expand(\"file_{sample}.txt\",\n               sample=[\"1\",\"2\",\"3\"])\n\nrule create_file:\n    output:\n        \"file_{sample}.txt\"\n    shell:\n        \"\"\"\n        touch {output}\n        \"\"\"\nIn this case, Snakemake will create a workflow that looks like this:\n\n\n\n\nflowchart LR\n  A[file_1] --&gt; F(all)\n  B[file_2] --&gt; F(all)\n  C[file_3] --&gt; F(all)\n\n\n\n\n\n\n\nAbout the syntax\nAll you need for a wildcard is a list, which you pass to the expand() function. The first argument in the expand function is your filepath string, with the wildcard in {curly brackets}. Then, you pass the list to use for expanding the filepath. In the previous example, the sample=[\"1\",\"2\",\"3\"] defines the values of the wildcard sample in file_{sample}.txt.\n\n\nMultiple wildcards\nThis doesn’t have to be sample, and you can have multiple wildcards in an expand() function. Imagine you’re plating a 5-course meal for 3 people - your snakefile would look like this:\ncourse_numbers=[\"1\",\"2\",\"3\",\"4\",\"5\"]\npeople=[\"John\", \"Madi\", \"Casey\"]\n\nrule all:\n    expand(\"course_{course_number}_for_{person}.plate\",\n           course_number=course_numbers,\n           person=people)\n\nrule make_food:\n    ouput:\n        \"course_{course_number}_for_{person}.plate\"\n    shell:\n        \"\"\"\n        make food...\n        \"\"\""
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#wildcards-to-run-snakemake-rules-for-each-sample",
    "href": "quarto/Snakemake-Essentials.html#wildcards-to-run-snakemake-rules-for-each-sample",
    "title": "Snakemake Essentials",
    "section": "Wildcards to run snakemake rules for each sample",
    "text": "Wildcards to run snakemake rules for each sample\nThis is a very common use for wildcards! We often use wildcards we want to run the same rules for each sample. This is one of the ways Snakemake starts to shine. If you have a metadata file with 500 sample IDs, you can read that list of sample IDs into Snakemake using Python/Pandas, then run your snakemake pipeline for all samples. This is what that looks like:\nimport pandas as pd\nmetadata = pd.read_csv(\"metadata.csv\")\nsamples = metadata[\"SampleID\"] # Samples are in a column named SampleID\n\nrule all:\n    input:\n        expand(\"file_{sample}.txt\",\n               sample=samples)\n\nrule create_file:\n    output:\n        \"file_{sample}.txt\"\n    shell:\n        \"\"\"\n        touch {output}\n        \"\"\""
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#wildcards-for-parameter-exploration",
    "href": "quarto/Snakemake-Essentials.html#wildcards-for-parameter-exploration",
    "title": "Snakemake Essentials",
    "section": "Wildcards for parameter exploration",
    "text": "Wildcards for parameter exploration\nAnother common use of wildcards is to explore the effects of using different parameters in your analysis.\nLet’s look back to our example of trimming sequencing reads. If we wanted to look at changes in read quality if we trim each sample’s reads at different base positions, this is what our snakefile could look like:\nimport pandas as pd\nmetadata = pd.read_csv(\"metadata.csv\")\nsamples = metadata[\"SampleID\"] # Samples are in a column named SampleID\n\nread_trim_positions = [\"0\",\"10\",\"20\"]\nread_trunc_positions = [\"0\",\"10\",\"20\"] \n\nrule all:\n    input:\n        \"multiqc_report/multiqc_report.html\"\n\n# Trim fastq files with varying parameters\nrule trim_fastq:\n    input:\n        \"raw_reads_{sample}.fq\"\n    output:\n        \"trimmed_reads_{sample}_b{start}_e{end}.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b {wildcards.start} -e {wildcards.end} {input} &gt; {output}\n        \"\"\"\n\n# Run each of the trimmed fastq files through fastQC for quality control\nrule run_fastQC:\n    input:\n        \"trimmed_reads_{sample}_b{start}_e{end}.fq\"\n    output:\n        \"trimmed_reads_{sample}_b{start}_e{end}_fastqc.zip\"\n    shell:\n        \"\"\"\n        fastqc {input} -o .\n        \"\"\"\n\n# Aggregate fastQC reports using MultiQC\nrule run_multiQC:\n    input:\n        ### LOOK HERE, THIS IS OUR EXPAND COMMAND #################\n        expand(\"trimmed_reads_{sample}_b{start}_e{end}_fastqc.zip\",\n                sample=samples,\n                start=read_trim_positions,\n                end=read_trunc_positions)\n    output:\n        \"multiqc_report/multiqc_report.html\"\n    shell:\n        \"\"\"\n        multiqc . -o multiqc_report\n        \"\"\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Effective use of Snakemake for workflow management on high performance compute clusters",
    "section": "",
    "text": "Overview\nWelcome! This is a learning resource on how to effectively use the workflow management tool Snakemake to make your life easier when processing large amounts of data on high performance compute clusters.\n\n\nGoals\nHopefully, this resource will help you:\n\nUnderstand why workflow management tools are useful\nCreate generalizable workflows with Snakemake\nCreate reproducible workflows with Snakemake\nCreate Snakemake workflows that run on compute clusters in an efficient manner (parallelized job submission and resource management)\n\n\n\nApplications to shotgun metagenomics\nSome examples will be focused on dealing with high-throughput sequencing data, such as metagenomics, since this is what I (and the original audience of this resource) work with on a daily basis. However, the contents and principles still apply to other data types.\nA primer on shotgun metagenomic data:\n\nThe Fastq format is how we store sequencing data. It’s a (sometimes compressed) text file with the genome sequence and quality of the sequence.\nIf any of these concepts are unfamiliar to your, or if they spark an interest in working with metagenomic data:\n\nThomas Sharpton has written An introduction to the analysis of shotgun metagenomic data, which is a nice overview of many data processing steps.\nAdditionally, the Metagenomics Wiki explains many of the tools, as well as how to use them.\n\n\n\n\nApplications to 16S rDNA data\nOther examples may be based on the analysis of 16S rDNA sequencing data which is commonly performed for microbiome profiling and is what the other contributor to this resource works with on a daily basis. Once again, don’t fret if you’ve never encountered 16S rDNA data, the basic principles of Snakemake can be applied in a variety of situations.\nA brief background on 16S rDNA data:\n\nLike shotgun metagenomics, the sequencing data is stored in Fastq format as a compressed file.\nThese fastq.gz files are then put into microbiome profiling programs such as Qiime2 or mothur.\n\n\n\nContributing to this resource\nIf you have questions or suggestions, feel free to open an issue or pull request on the GitHub repo for this site :)"
  },
  {
    "objectID": "quarto/About-Snakemake.html",
    "href": "quarto/About-Snakemake.html",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake is a workflow management tool. That’s fancy words for “it makes your life easier when you have lots of different compute jobs to run.”\nAt a very surface level, Snakemake looks for the output files you need, and if they aren’t there, it figures out all the steps that need to be run in order to get those files.\nConsider the following example. If snakemake can’t find output file, it will run process data on the input data.\n\n\n\n\nflowchart LR\n  B[process data] --&gt; C(output file)\n  A(input data) --&gt; B[process data]\n  \n\n\n\n\n\n\n\nSounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and Snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but Snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier.\n\n\n\n\n\nSnakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  },
  {
    "objectID": "quarto/About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "href": "quarto/About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "title": "About Snakemake",
    "section": "",
    "text": "Sounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and Snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but Snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier."
  },
  {
    "objectID": "quarto/About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "href": "quarto/About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  },
  {
    "objectID": "quarto/Config-files.html",
    "href": "quarto/Config-files.html",
    "title": "Using Config Files",
    "section": "",
    "text": "Config files in Snakemake workflows are used to store and manage configuration parameters separately from the workflow script. Typically written in YAML format, these files contain key-value pairs representing various parameters such as file paths, resource requirements, and other settings that control the behavior of the workflow."
  },
  {
    "objectID": "quarto/Config-files.html#passing-a-config-file-to-snakemake",
    "href": "quarto/Config-files.html#passing-a-config-file-to-snakemake",
    "title": "Using Config Files",
    "section": "Passing a config file to Snakemake",
    "text": "Passing a config file to Snakemake\nTo run Snakemake with a config file, pass it after the flag --config in your Snakemake command, as such: snakemake --cores 1 --config config.yaml."
  },
  {
    "objectID": "quarto/Config-files.html#format",
    "href": "quarto/Config-files.html#format",
    "title": "Using Config Files",
    "section": "Format",
    "text": "Format\nConfig files should be YAML-formatted. Here is an example:\n# Input data file\ninput_file: data/input.txt\n\n# Output directory\noutput_file: data/processed_file.txt\n\n# Parameters for a specific rule\nspecific_rule_params:\n  param1: value1\n  param2: value2\n  \n# Number of threads to use for a specific rule\nspecific_rule_threads: 1"
  },
  {
    "objectID": "quarto/Config-files.html#snakefile",
    "href": "quarto/Config-files.html#snakefile",
    "title": "Using Config Files",
    "section": "Snakefile",
    "text": "Snakefile\nIf the config filepath is passed to Snakemake, it will automatically be read into your snakefile as a dictionary object named config. The below example shows how you can use this config dictionary in your rules. I recommend not trying to access exact components of the config file inside your shell command (or run command), but instead making those components params that can be accessed more traditionally.\nrule all:\n    input:\n        config[\"output_file\"]\n\n# Rule to process the input file\nrule specific_rule:\n    input:\n        config[\"input_file\"]\n    output:\n        config[\"output_file\"]\n    threads: config[\"specific_rule_threads\"]\n    params:\n        param1=config[\"specific_rule_params\"][\"param1\"],\n        param2=config[\"specific_rule_params\"][\"param2\"]\n    shell:\n        \"\"\"\n        cp {input} {output}\n        echo {params.param1} &gt;&gt; {output}\n        echo {params.param2} &gt;&gt; {output}\n        \"\"\"\n\nSafely handling config params\nThe way that the config dictionary’s values are accessed above works, but it isn’t necessarily the most user-friendly way to access the values. I’m not saying this has happened to me, but if a jealous ex-lover sneaks into your house on a Tuesday night and deletes parameters out of your config file, the previously shown dictionary value accession method will result in errors in your Snakemake pipeline. For someone unfamiliar with the pipeline, it may be very difficult to track down that these errors are coming from a missing parameter in the config file.\nIn some cases, it can be better to use the dict.get() syntax, or even to write helper functions to sanitize your config. With dict.get(), you can set default values. Using config.get(\"specific_rule_threads\", default=1) would look for specific_rule_threads in your config, but if it doesn’t exist, Snakemake would default to using 1 thread for this rule. In contrast, config[\"specific_rule_threads\"] would raise a KeyError if specific_rule_threads is not in the config.\nYou may imagine some cases where defaulting to a set value could be preferable. For example, if you want a rule to default to using 1 thread, unless a user specificies otherwise, the get() syntax could be great. However, if the entire pipeline should error if a user doesn’t provide an input filepath, the get() syntax may not be the ideal solution. If a pipeline should error if a user doesn’t provide a certain parameter in the config, this is best handled with an informative helper function to sanitize. Below is an simple example:\n\ndef check_keys_exist(config, keys):\n    \"\"\"\n    Parameters:\n    - dictionary (dict): The config dictionary to check.\n    - keys (set): The set of keys to check for.\n\n    Returns:\n    - bool: True if all keys are present\n    \n    If any key is missing, a specific ValueError will be raised\n    \"\"\"\n    for key in keys:\n        if key not in config:\n            raise ValueError(f\"{key} is missing from your config file. Please ensure this parameter is present\")\n    \n    return True\n\nAdditionally, you may need to check certain aspects of the config parameters. Did the user add quotes around the values? Are there underscores in a parameter that should have dashes instead. These are things to think about if other people will be using your Snakemake pipeline."
  },
  {
    "objectID": "quarto/Using-R-with-Snakemake.html",
    "href": "quarto/Using-R-with-Snakemake.html",
    "title": "Using R with Snakemake",
    "section": "",
    "text": "Running R with Snakemake works beautifully and makes your life easier - if you’ve already gone through the pains of setting up your R to be compatible with the scripts you want to run.\n\n\nAll of your R packages (called libraries in R once installed). If you’re running Snakemake locally, it will use R and its packages already installed on your computer, so there’s no problem. BUT if you’re running Snakemake elsewhere, such as a super computer or any other computer that isn’t yours, this becomes a massive issue.\n\n\n1. Creating an R script that you run first that tells R to install all the packages that you will need. JOHN\n2. Creating an R-specific conda environment, installing all needed packages into that environment, and referencing it in your snakefile.\nInstalling your desired R packages through conda is a lot faster than if you were to install them via R (which seems semi counter-intuitive) and the process is fairly fool proof. You can set up your R environment in a .yaml file - similar to your other environments. For example:\nname: r_env\nchannels:\n    - bioconda\n    - conda-forge\n    - anaconda\n    - r\ndependencies:\n    - pip\n    - r-base=&gt; 4.0\n    - r-essentials\n    - r-qiime2r\n    - r-rstatix\n    - r-ggpubr\n    - r-cowplot\n    - r-ggh4x\n    - r-argparse\n    - pip:\n        - snakemake\nYou need to make sure that you’re keeping track of all needed R packages and putting them in your .yaml file. I would also reccommend validating the proper channels and dependencies for the R packages that you need at the Anaconda website by looking up the package and reading the conda installation instructions. Be warned that not all R packages can be installed through conda channels.\nOnce your .yaml file is set up, you can run:\nconda env create -f r_env.yml\nEt viola! Your R-specific conda environment has successfully been installed!"
  },
  {
    "objectID": "quarto/Using-R-with-Snakemake.html#the-issue",
    "href": "quarto/Using-R-with-Snakemake.html#the-issue",
    "title": "Using R with Snakemake",
    "section": "",
    "text": "All of your R packages (called libraries in R once installed). If you’re running Snakemake locally, it will use R and its packages already installed on your computer, so there’s no problem. BUT if you’re running Snakemake elsewhere, such as a super computer or any other computer that isn’t yours, this becomes a massive issue.\n\n\n1. Creating an R script that you run first that tells R to install all the packages that you will need. JOHN\n2. Creating an R-specific conda environment, installing all needed packages into that environment, and referencing it in your snakefile.\nInstalling your desired R packages through conda is a lot faster than if you were to install them via R (which seems semi counter-intuitive) and the process is fairly fool proof. You can set up your R environment in a .yaml file - similar to your other environments. For example:\nname: r_env\nchannels:\n    - bioconda\n    - conda-forge\n    - anaconda\n    - r\ndependencies:\n    - pip\n    - r-base=&gt; 4.0\n    - r-essentials\n    - r-qiime2r\n    - r-rstatix\n    - r-ggpubr\n    - r-cowplot\n    - r-ggh4x\n    - r-argparse\n    - pip:\n        - snakemake\nYou need to make sure that you’re keeping track of all needed R packages and putting them in your .yaml file. I would also reccommend validating the proper channels and dependencies for the R packages that you need at the Anaconda website by looking up the package and reading the conda installation instructions. Be warned that not all R packages can be installed through conda channels.\nOnce your .yaml file is set up, you can run:\nconda env create -f r_env.yml\nEt viola! Your R-specific conda environment has successfully been installed!"
  }
]