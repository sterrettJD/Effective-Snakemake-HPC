[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Effective use of Snakemake for workflow management on high performance compute clusters",
    "section": "",
    "text": "Overview\nWelcome! This is a learning resource on how to effectively use the workflow management tool Snakemake to make your life easier when processing large amounts of data on high performance compute clusters.\n\n\nGoals\nHopefully, this resource will help you:\n\nUnderstand why workflow management tools are useful\nCreate generalizable workflows with Snakemake\nCreate reproducible workflows with Snakemake\nCreate Snakemake workflows that run on compute clusters in an efficient manner (parallelized job submission and resource management)\n\n\n\nApplications to shotgun metagenomics\nSome examples will be focused on dealing with high-throughput sequencing data, such as metagenomics, since this is what I (and the original audience of this resource) work with on a daily basis. However, the contents and principles still apply to other data types.\nA primer on shotgun metagenomic data:\n\nThe Fastq format is how we store sequencing data. It’s a (sometimes compressed) text file with the genome sequence and quality of the sequence.\nIf any of these concepts are unfamiliar to your, or if they spark an interest in working with metagenomic data:\n\nThomas Sharpton has written An introduction to the analysis of shotgun metagenomic data, which is a nice overview of many data processing steps.\nAdditionally, the Metagenomics Wiki explains many of the tools, as well as how to use them.\n\n\n\n\nApplications to 16S rDNA data\nOther examples may be based on the analysis of 16S rDNA sequencing data which is commonly performed for microbiome profiling and is what the other contributor to this resource works with on a daily basis. Once again, don’t fret if you’ve never encountered 16S rDNA data, the basic principles of Snakemake can be applied in a variety of situations.\nA brief background on 16S rDNA data:\n\nLike shotgun metagenomics, the sequencing data is stored in Fastq format as a compressed file.\nThese fastq.gz files are then put into microbiome profiling programs such as Qiime2 or mothur.\n\n\n\nContributing to this resource\nIf you have questions or suggestions, feel free to open an issue or pull request on the GitHub repo for this site :)"
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html",
    "href": "quarto/Snakemake-Essentials.html",
    "title": "Snakemake Essentials",
    "section": "",
    "text": "Installation instructions can be found on the Snakemake documentation page here. In short, you want to install conda and mamba-forge, then run:\nmamba create -c conda-forge -c bioconda -n snakemake_env snakemake\nAlternatively, if you want to install Snakemake in an environment you already have set up, first activate that environment, then run:\npip install snakemake\nThis is especially important if you will be using more than one conda environment in your pipeline."
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#what-is-rule-all",
    "href": "quarto/Snakemake-Essentials.html#what-is-rule-all",
    "title": "Snakemake Essentials",
    "section": "What is “rule all”?",
    "text": "What is “rule all”?\nAt this point, you may be asking “Wait! You said I should have a rule for each step! What’s this rule all mess??”\nrule all is how we specify the output files we want, and it’s located at the very top of your snakefile. You can specify target files (pipeline endpoints) using the input to rule all. In a scientific case, consider these files to be the input to the paper you’ll write from your analysis.\nSnakemake is tracking what rules need to be run in order to generate the inputs for other rules, so it will track that - rule run_multiQC’s outputs -&gt; rule all’s inputs - rule run_fastQC’s outputs -&gt; rule run_multiQC’s inputs - rule trim_fastq’s outputs -&gt; rule run_fastQC’s inputs\nTherefore, it knows that in order to have the final files, it will use rule trim_fastq -&gt; rule run_fastQC -&gt; rule run_multiQC -&gt; rule all\n\n\n\n\nflowchart LR\n  C[run_multiQC] --&gt; D(all)\n  B[run_fastQC] --&gt; C[run_multiQC]\n  A[trim_fastq] --&gt; B[run_fastQC]"
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#structure-of-rules",
    "href": "quarto/Snakemake-Essentials.html#structure-of-rules",
    "title": "Snakemake Essentials",
    "section": "Structure of rules",
    "text": "Structure of rules\nRules provide crucial information to Snakemake, such as a step’s inputs, output, and the command to run. These are the bare bones of each rule, but as we develop more, we will start to also include aspects of each rule, including the Conda environment, resource requirements (time and memory), and other parameters.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    shell:\n        \"\"\"\n        # do things to input file to make ouput file\n        \"\"\"\nInstead of shell:, users can also use run: which will run Python code.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    run:\n        # python things ..."
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#inputs-and-outputs",
    "href": "quarto/Snakemake-Essentials.html#inputs-and-outputs",
    "title": "Snakemake Essentials",
    "section": "Inputs and outputs",
    "text": "Inputs and outputs\nSnakemake traces the inputs and outputs for each rule to know what rules need to be run (and what order to run them in). These are specified very explicitly in the rule, using input: and output:, followed by an indented, comma-separated list, with one entry per line. These can also be named in the list. Another great attribute of Snakemake is that these can be referenced in the command it runs.\n\nExamples\n\n1 input, 1 output\nrule rename_file:\n    input:\n        \"old_name.txt\"\n    output:\n        \"new_name.txt\"\n    shell:\n        \"\"\"\n        mv {input} {output}\n        # same as running\n        # mv old_name.txt new_name.txt\n        \"\"\"\n\n\n2 named inputs, 2 named outputs\nrule rename_multiple_files:\n    input:\n        file_1=\"first_file.txt\",\n        file_2=\"second_file.txt\"\n    output:\n        file_1=\"file_1.txt\",\n        file_2=\"file_2.txt\"\n    shell:\n        \"\"\"\n        mv {input.file_1} {output.file_1}\n        mv {input.file_2} {output.file_2}\n        \"\"\""
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#example-with-trimming-reads",
    "href": "quarto/Snakemake-Essentials.html#example-with-trimming-reads",
    "title": "Snakemake Essentials",
    "section": "Example with trimming reads",
    "text": "Example with trimming reads\n\nSimple\nImagine you want to trim one fastq file (10 base pairs from the beginning, 5 base pairs from the end) using SeqTK. This is what a very simple snakefile could look like:\nrule all:\n    input:\n        \"trimmed_reads.fq\"\n\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\n\n\nChaining rules\nNow, imagine the raw reads are compressed. We want to unzip them, trim the reads, and recompress them. You could do this in 1 step, but let’s break it up for the sake of learning. That would look like this:\nrule all:\n    input:\n        \"trimmed_reads.fq.gz\"\n\nrule unzip_fastq:\n    input:\n        \"raw_reads.fq.gz\"\n    output:\n        \"raw_reads.fq\"\n    shell:\n        \"\"\"\n        gunzip {input}\n        \"\"\"\n\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\n\nrule zip_trimmed:\n    input:\n        \"trimmed_reads.fq\"\n    output:\n        \"trimmed_reads.fq.gz\"\n    shell:\n        \"\"\"\n        gzip {input}\n        \"\"\"\nWhich would create a workflow like this:\n\n\n\n\nflowchart LR\n  C[zip_trimmed] --&gt; D(all)\n  B[trim_fastq] --&gt; C[zip_trimmed]\n  A[unzip_fastq] --&gt; B[trim_fastq]\n\n\n\n\n\n(We will build on this example)"
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#about-wildcards",
    "href": "quarto/Snakemake-Essentials.html#about-wildcards",
    "title": "Snakemake Essentials",
    "section": "About wildcards",
    "text": "About wildcards\nWildcards are a big part of how we can expand and generalize how our snakemake pipeline works. Consider a wildcards to be a list of values for which you want to run a snakemake rule multiple times. This is a bit like a for loop:\n\nfor value in wildcards: \n    run rule..."
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#syntax",
    "href": "quarto/Snakemake-Essentials.html#syntax",
    "title": "Snakemake Essentials",
    "section": "Syntax",
    "text": "Syntax\n\nOne wildcard\nWe can expand a rule using wildcards by expand()ing the input to a rule that requires this rule’s output. For example:\nrule all:\n    input:\n        expand(\"file_{sample}.txt\",\n               sample=[\"1\",\"2\",\"3\"])\n\nrule create_file:\n    output:\n        \"file_{sample}.txt\"\n    shell:\n        \"\"\"\n        touch {output}\n        \"\"\"\nIn this case, Snakemake will create a workflow that looks like this:\n\n\n\n\nflowchart LR\n  A[file_1] --&gt; F(all)\n  B[file_2] --&gt; F(all)\n  C[file_3] --&gt; F(all)\n\n\n\n\n\n\n\nAbout the syntax\nAll you need for a wildcard is a list, which you pass to the expand() function. The first argument in the expand function is your filepath string, with the wildcard in {curly brackets}. Then, you pass the list to use for expanding the filepath. In the previous example, the sample=[\"1\",\"2\",\"3\"] defines the values of the wildcard sample in file_{sample}.txt.\n\n\nMultiple wildcards\nThis doesn’t have to be sample, and you can have multiple wildcards in an expand() function. Imagine you’re plating a 5-course meal for 3 people - your snakefile would look like this:\ncourse_numbers=[\"1\",\"2\",\"3\",\"4\",\"5\"]\npeople=[\"John\", \"Madi\", \"Casey\"]\n\nrule all:\n    expand(\"course_{course_number}_for_{person}.plate\",\n           course_number=course_numbers,\n           person=people)\n\nrule make_food:\n    ouput:\n        \"course_{course_number}_for_{person}.plate\"\n    shell:\n        \"\"\"\n        make food...\n        \"\"\""
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#wildcards-to-run-snakemake-rules-for-each-sample",
    "href": "quarto/Snakemake-Essentials.html#wildcards-to-run-snakemake-rules-for-each-sample",
    "title": "Snakemake Essentials",
    "section": "Wildcards to run snakemake rules for each sample",
    "text": "Wildcards to run snakemake rules for each sample\nThis is a very common use for wildcards! We often use wildcards we want to run the same rules for each sample. This is one of the ways Snakemake starts to shine. If you have a metadata file with 500 sample IDs, you can read that list of sample IDs into Snakemake using Python/Pandas, then run your snakemake pipeline for all samples. This is what that looks like:\nimport pandas as pd\nmetadata = pd.read_csv(\"metadata.csv\")\nsamples = metadata[\"SampleID\"] # Samples are in a column named SampleID\n\nrule all:\n    input:\n        expand(\"file_{sample}.txt\",\n               sample=samples)\n\nrule create_file:\n    output:\n        \"file_{sample}.txt\"\n    shell:\n        \"\"\"\n        touch {output}\n        \"\"\""
  },
  {
    "objectID": "quarto/Snakemake-Essentials.html#wildcards-for-parameter-exploration",
    "href": "quarto/Snakemake-Essentials.html#wildcards-for-parameter-exploration",
    "title": "Snakemake Essentials",
    "section": "Wildcards for parameter exploration",
    "text": "Wildcards for parameter exploration\nAnother common use of wildcards is to explore the effects of using different parameters in your analysis.\nLet’s look back to our example of trimming sequencing reads. If we wanted to look at changes in read quality if we trim each sample’s reads at different base positions, this is what our snakefile could look like:\nimport pandas as pd\nmetadata = pd.read_csv(\"metadata.csv\")\nsamples = metadata[\"SampleID\"] # Samples are in a column named SampleID\n\nread_trim_positions = [\"0\",\"10\",\"20\"]\nread_trunc_positions = [\"0\",\"10\",\"20\"] \n\nrule all:\n    input:\n        \"multiqc_report/multiqc_report.html\"\n\n# Trim fastq files with varying parameters\nrule trim_fastq:\n    input:\n        \"raw_reads_{sample}.fq\"\n    output:\n        \"trimmed_reads_{sample}_b{start}_e{end}.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b {wildcards.start} -e {wildcards.end} {input} &gt; {output}\n        \"\"\"\n\n# Run each of the trimmed fastq files through fastQC for quality control\nrule run_fastQC:\n    input:\n        \"trimmed_reads_{sample}_b{start}_e{end}.fq\"\n    output:\n        \"trimmed_reads_{sample}_b{start}_e{end}_fastqc.zip\"\n    shell:\n        \"\"\"\n        fastqc {input} -o .\n        \"\"\"\n\n# Aggregate fastQC reports using MultiQC\nrule run_multiQC:\n    input:\n        ### LOOK HERE, THIS IS OUR EXPAND COMMAND #################\n        expand(\"trimmed_reads_{sample}_b{start}_e{end}_fastqc.zip\",\n                sample=samples,\n                start=read_trim_positions,\n                end=read_trunc_positions)\n    output:\n        \"multiqc_report/multiqc_report.html\"\n    shell:\n        \"\"\"\n        multiqc . -o multiqc_report\n        \"\"\""
  },
  {
    "objectID": "quarto/Qiime2-Applications.html",
    "href": "quarto/Qiime2-Applications.html",
    "title": "Qiime2 Applications",
    "section": "",
    "text": "Firstly, let’s make sure that you have conda, mamba-forge, and Qiime2 installed. Follow the installation instructions for the operating system of your local computer (or wherever you want to install Qiime2) which can be found here.\nOnce you have Qiime2 installed, activate your Qiime2 environment by running conda activate qiime2-2023.5 before running:\npip install snakemake\nYou now have Snakemake installed in your Qiime2 environment and can start working on your pipeline!"
  },
  {
    "objectID": "quarto/Qiime2-Applications.html#things-ive-had-to-learn-the-hard-way",
    "href": "quarto/Qiime2-Applications.html#things-ive-had-to-learn-the-hard-way",
    "title": "Qiime2 Applications",
    "section": "Things I’ve had to learn the hard way:",
    "text": "Things I’ve had to learn the hard way:\n\nAnytime you call a Qiime2 command that outputs a directory of files (like qiime diversity core-metrics-phylogenetic), Qiime and Snakemake get mad at each other because Snakemake creates the output directory before Qiime. Qiime will then give you an error about how the output directory already exists (because Snakemake created it first).\n\nInstead, you need to specify that the output you’re generating in your shell is an entire directory by using output = directory(\"directory_name\"). For example:\nrule all:\n    input:\n        # can reference output directory like so here\n        \"core_metrics_directory\"\n\n\nrule core_metrics_analysis:\n    input:\n        TREE=\"tree.qza\",\n        TABLE=\"tax_filt_actual.qza\",\n        METADATA=\"metadata.tsv\"\n    output:\n        OUTPUT_DIR=directory(\"core_metrics_directory\")\n    params:\n        sampling_depth=10000\n    shell:\n        \"\"\"\n        qiime diversity core-metrics-phylogenetic \\\n            --i-phylogeny {input.TREE} \\\n            --i-table {input.TABLE} \\\n            --p-sampling-depth {params.sampling_depth} \\\n            --m-metadata-file {input.METADATA} \\\n            --output-dir {output} \n        \"\"\"\n\nI used to list every single output file that would be in that directory in the Qiime command itself instead of just the output directory. This also works but it get’s a bit annoying after a while, especially if the output directory being generated has file names that change with each run. For example:\nrule core_metrics_analysis:\n    input:\n        TREE=\"tree.qza\",\n        TABLE=\"tax_filt_actual.qza\",\n        METADATA=\"metadata.tsv\"\n    output:\n        BCDM=\"bray_curtis_distance_matrix.qza\",\n        BCEMP=\"bray_curtis_emperor.qzv\",\n        BCPCOA=\"bray_curtis_pcoa_results.qza\",\n        EVEN=\"evenness_vector.qza\",\n        FAITH=\"faith_pd_vector.qza\",\n        JDM=\"jaccard_distance_matrix.qza\",\n        JEMP=\"jaccard_emperor.qzv\",\n        JPCOA=\"jaccard_pcoa_results.qza\",\n        OF=\"observed_features_vector.qza\",\n        RAREFIED=\"rarefied_table.qza\",\n        SHANNON=\"shannon_vector.qza\",\n        UUDM=\"unweighted_unifrac_distance_matrix.qza\",\n        UUEMP=\"unweighted_unifrac_emperor.qzv\",\n        UUPCOA=\"unweighted_unifrac_pcoa_results.qza\",\n        WUDM=\"weighted_unifrac_distance_matrix.qza\",\n        WUEMP=\"weighted_unifrac_emperor.qzv\",\n        WUPCOA=\"weighted_unifrac_pcoa_results.qza\"\n    conda:\n        \"qiime2-2023.5\"\n    params:\n        sampling_depth=10000\n    shell:\n        \"\"\"\n        qiime diversity core-metrics-phylogenetic \\\n            --i-phylogeny {input.TREE} \\\n            --i-table {input.TABLE} \\\n            --p-sampling-depth {params.sampling_depth} \\\n            --m-metadata-file {input.METADATA} \\\n            --o-rarefied-table {output.RAREFIED} \\\n            --o-faith-pd-vector {output.FAITH} \\\n            --o-observed-features-vector {output.OF} \\\n            --o-shannon-vector {output.SHANNON} \\\n            --o-evenness-vector {output.EVEN} \\\n            --o-unweighted-unifrac-distance-matrix {output.UUDM} \\\n            --o-weighted-unifrac-distance-matrix {output.WUDM} \\\n            --o-jaccard-distance-matrix {output.JDM} \\\n            --o-bray-curtis-distance-matrix {output.BCDM} \\\n            --o-unweighted-unifrac-pcoa-results {output.UUPCOA} \\\n            --o-weighted-unifrac-pcoa-results {output.WUPCOA} \\\n            --o-jaccard-pcoa-results {output.JPCOA} \\\n            --o-bray-curtis-pcoa-results {output.BCPCOA} \\\n            --o-unweighted-unifrac-emperor {output.UUEMP} \\\n            --o-weighted-unifrac-emperor {output.WUEMP} \\\n            --o-jaccard-emperor {output.JEMP} \\\n            --o-bray-curtis-emperor {output.BCEMP}\n        \"\"\""
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html",
    "href": "quarto/Cluster-friendly-Snakemake.html",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Compute clusters are often shared resources. For example, Fiji and Alpine are two high performance compute clusters for researchers at the University of Colorado. These resources are shared across the entire university (and sometimes multiple universities), which makes managing resources very important.\n\n\n\nResource management for computing is often done using a job scheduler such as Slurm. Slurm will be the focal job scheduling system in this resource, but others exist (the concepts remain consistent across other systems). Slurm manages the priority of compute jobs across users (based on factors such as memory/time requirements and user priority), and it manages parallelization and execution of jobs on compute nodes.\n\n\n\nUnlike running commands from a command line on your local computer, any compute-heavy jobs on clusters need to be done through the submission of batch jobs. Batch jobs could be considered “wrappers” for your compute jobs, with specifications of the resources needed. Slurm allocates the necessary resources on a compute node, then sends that job to be performed on the compute node. Below is an image from ETH Zurich scientific computing wiki demonstrating this process. \n\n\n\nThis often involved running a .sbatch script, which has the resource requirements at the top. Here is an example for trimming reads in a fastq file:\n#SBATCH --mail-user=&lt;email_address&gt;@colorado.edu\n#SBATCH --partition=short # run on the short partition\n#SBATCH --nodes=1 # Only use a single node\n#SBATCH --ntasks=1 # Run with one thread\n#SBATCH --mem=8gb # Memory limit\n#SBATCH --time=02:00:00 # Time limit hrs:min:sec\n#SBATCH --output=/path/to/here/logs/trim_reads_%j.out # Standard output and error log\n#SBATCH --error=/path/to/here/logs/trim_reads_%j.err # %j inserts job number\n\n# activate conda environment\nsource activate seqtk_env\n\n# trim reads\nseqtk trimfq -b 10 -e 5 raw_reads.fq &gt; trimmed_reads.fq\nWhich could be run with the following command:\nsbatch trim_reads.sbatch\n\n\n\nThis works well for single jobs, but again, when things start to scale up (50 samples, 10 steps per sample, etc..), this can become cumbersome.\n\n\nWhen I was new to working on clusters, I found myself writing python scripts to loop through each of my samples, find their files, then run sbatch scripts that had to parse/sanitize multiple parameters. I certainly could have done this more simply, but it quickly snowballed into a 50-150 line python script with a &gt;25 line sbatch script per step, leading to a &gt;&gt;1000 line codebase for a project with just a few processing steps. The last straw was when I needed to go back and run step 1 with new parameters, meaning I had to manually rerun every following step (after waiting for all samples to finish each step).\nUsing Snakemake, to manage I was able to reduce this codebase by ~70% and reduce my manual input by ~50%. To quote Casey Martin, “Compute flops are cheap and nearly unlimited at our level; human flops are expensive and limited,” so we need to prioritize efficiency in our human flops, using tools like Snakemake.\n\n\n\nMy favorite part of using Snakemake has been that once a Slurm profile is set up, it will handle all of the sbatch job submission for you. This “profile” is really just a little tool that creates the .sbatch script, submits it, logs output/error, and monitors completion. In your Snakefile, you can specify resource requirements and conda environment, such that the previous script would look like this:\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"short\",\n        mem_mb=8000,\n        runtime=120 #min\n    threads: 1\n    conda: \"seqtk_env\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\nIn this case it doesn’t look much more concise, but it is much easier to generalize this rule to run multiple samples/parameters, and to chain multiple samples together. Most importantly, it’s easier to read and debug!\n\n“Programs must be written for people to read, and only incidentally for machines to execute.” ― Harold Abelson, Structure and Interpretation of Computer Programs"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#shared-resource",
    "href": "quarto/Cluster-friendly-Snakemake.html#shared-resource",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Compute clusters are often shared resources. For example, Fiji and Alpine are two high performance compute clusters for researchers at the University of Colorado. These resources are shared across the entire university (and sometimes multiple universities), which makes managing resources very important."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#slurm",
    "href": "quarto/Cluster-friendly-Snakemake.html#slurm",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Resource management for computing is often done using a job scheduler such as Slurm. Slurm will be the focal job scheduling system in this resource, but others exist (the concepts remain consistent across other systems). Slurm manages the priority of compute jobs across users (based on factors such as memory/time requirements and user priority), and it manages parallelization and execution of jobs on compute nodes."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#batch-jobs",
    "href": "quarto/Cluster-friendly-Snakemake.html#batch-jobs",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "Unlike running commands from a command line on your local computer, any compute-heavy jobs on clusters need to be done through the submission of batch jobs. Batch jobs could be considered “wrappers” for your compute jobs, with specifications of the resources needed. Slurm allocates the necessary resources on a compute node, then sends that job to be performed on the compute node. Below is an image from ETH Zurich scientific computing wiki demonstrating this process."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#example-batch-jobs-submission",
    "href": "quarto/Cluster-friendly-Snakemake.html#example-batch-jobs-submission",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "This often involved running a .sbatch script, which has the resource requirements at the top. Here is an example for trimming reads in a fastq file:\n#SBATCH --mail-user=&lt;email_address&gt;@colorado.edu\n#SBATCH --partition=short # run on the short partition\n#SBATCH --nodes=1 # Only use a single node\n#SBATCH --ntasks=1 # Run with one thread\n#SBATCH --mem=8gb # Memory limit\n#SBATCH --time=02:00:00 # Time limit hrs:min:sec\n#SBATCH --output=/path/to/here/logs/trim_reads_%j.out # Standard output and error log\n#SBATCH --error=/path/to/here/logs/trim_reads_%j.err # %j inserts job number\n\n# activate conda environment\nsource activate seqtk_env\n\n# trim reads\nseqtk trimfq -b 10 -e 5 raw_reads.fq &gt; trimmed_reads.fq\nWhich could be run with the following command:\nsbatch trim_reads.sbatch"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#using-snakemake-to-automate-batch-job-submission",
    "href": "quarto/Cluster-friendly-Snakemake.html#using-snakemake-to-automate-batch-job-submission",
    "title": "Cluster-friendly Snakemake",
    "section": "",
    "text": "This works well for single jobs, but again, when things start to scale up (50 samples, 10 steps per sample, etc..), this can become cumbersome.\n\n\nWhen I was new to working on clusters, I found myself writing python scripts to loop through each of my samples, find their files, then run sbatch scripts that had to parse/sanitize multiple parameters. I certainly could have done this more simply, but it quickly snowballed into a 50-150 line python script with a &gt;25 line sbatch script per step, leading to a &gt;&gt;1000 line codebase for a project with just a few processing steps. The last straw was when I needed to go back and run step 1 with new parameters, meaning I had to manually rerun every following step (after waiting for all samples to finish each step).\nUsing Snakemake, to manage I was able to reduce this codebase by ~70% and reduce my manual input by ~50%. To quote Casey Martin, “Compute flops are cheap and nearly unlimited at our level; human flops are expensive and limited,” so we need to prioritize efficiency in our human flops, using tools like Snakemake.\n\n\n\nMy favorite part of using Snakemake has been that once a Slurm profile is set up, it will handle all of the sbatch job submission for you. This “profile” is really just a little tool that creates the .sbatch script, submits it, logs output/error, and monitors completion. In your Snakefile, you can specify resource requirements and conda environment, such that the previous script would look like this:\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"short\",\n        mem_mb=8000,\n        runtime=120 #min\n    threads: 1\n    conda: \"seqtk_env\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\nIn this case it doesn’t look much more concise, but it is much easier to generalize this rule to run multiple samples/parameters, and to chain multiple samples together. Most importantly, it’s easier to read and debug!\n\n“Programs must be written for people to read, and only incidentally for machines to execute.” ― Harold Abelson, Structure and Interpretation of Computer Programs"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#why-it-matters",
    "href": "quarto/Cluster-friendly-Snakemake.html#why-it-matters",
    "title": "Cluster-friendly Snakemake",
    "section": "Why it matters",
    "text": "Why it matters\nSlurm profiles are advantageous because they manage submission/monitoring of each step in your Snakemake pipeline as separate batch jobs. For example, if you have 50 samples to process through 10 rules, the SLURM profile will submit/monitor each rule & sample combination as a separate job, so:\n\nThese jobs can run in parallel, with unique resource requirements for each job.\nIt is easy to monitor the job status for each unique sample/step.\nIf one fails, the others will keep going.\n\nWithout a Slurm profile, Snakemake would try to run each job sequentially within the batch job that you’ve submitted to run Snakemake, and this could get cumbersome. Additionally, imagine step 1 requires 2 GB of memory, but step 2 requires 100 GB memory. Without submitting individual batch jobs for each step/sample, you would have to request 100 GB memory for the full duration, which would result in a waste of resources on the shared cluster while the first step is running."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#using-cookiecutter",
    "href": "quarto/Cluster-friendly-Snakemake.html#using-cookiecutter",
    "title": "Cluster-friendly Snakemake",
    "section": "Using cookiecutter",
    "text": "Using cookiecutter\n\nAbout the profile\nI recommend using the official Snakemake Slurm profile, which you can install using cookiecutter. This profile manages job submission and monitors submitted jobs.\nIf you plan on running a Snakemake pipeline with many many steps that do not take very long to run (i.e., you’re submitting tens of thousands of jobs that run quickly, such that the submission is the bottleneck), you may want to check out John Blischak’s simple Snakemake Slurm profile, which is does not have as many status checks and submits jobs much faster. If job submission isn’t a bottleneck, it’s best to use the official Snakemake SLURM profile (use speed with caution).\n\n\nInstallating cookiecutter\nIn your Snakemake Conda environment:\npip install pipx\npipx install cookiecutter\n\n\nSnakemake SLURM profile setup\nIn your Snakemake Conda environment:\n# create config directory that snakemake searches for profiles (or use something else)\nprofile_dir=\"${HOME}/.config/snakemake\"\nmkdir -p \"$profile_dir\"\n# use cookiecutter to create the profile in the config directory\ntemplate=\"gh:Snakemake-Profiles/slurm\"\ncookiecutter --output-dir \"$profile_dir\" \"$template\"\nThen, hit enter to follow all of the default prompts, except make sure use_conda is True. That will look like this:"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#tell-snakemake-about-your-profile",
    "href": "quarto/Cluster-friendly-Snakemake.html#tell-snakemake-about-your-profile",
    "title": "Cluster-friendly Snakemake",
    "section": "Tell Snakemake about your profile",
    "text": "Tell Snakemake about your profile\nWhen running Snakemake with a profile, you need to use the --profile flag, like this:\nsnakemake --profile ~/.config/snakemake/slurm"
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#providing-resources-in-rules",
    "href": "quarto/Cluster-friendly-Snakemake.html#providing-resources-in-rules",
    "title": "Cluster-friendly Snakemake",
    "section": "Providing resources in rules",
    "text": "Providing resources in rules\nIn your rule, you can pass resources much like passing input/outputs specs. Threads are generally passed outside of the resources section. Additionally, any specifications for a rule can be accessed in the rule’s shell command, using syntax such as {resources.partition} to access the partition name, or {threads} to access the number of threads. Typically, you shouldn’t need to access Slurm-specific aspects like the partition (your profile is handling this), but for some command line tools, where you can specifiy resources, it’s useful to pass {threads} into your shell command.\nYou can consult the profile’s documentation for all of the options, but this is how I pass my resources.\nrule parallelizable_job:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"&lt;partition_name&gt;\",\n        mem_mb=int(8*1000), # 8 GB\n        runtime=int(2*60), # min, or 2 hours\n        slurm=\"mail-user=&lt;email_address&gt;@colorado.edu\"\n    threads: 8\n    shell:\n        \"\"\"\n        run_parallelizable_task --threads {threads} --max_memory {resources.mem_mb}\n        \"\"\"\nAdditionally, you can generally pass any “long format” names for Slurm parameters in your resources.slurm string. More details on that formatting can be found here."
  },
  {
    "objectID": "quarto/Cluster-friendly-Snakemake.html#rules-from-a-config-file",
    "href": "quarto/Cluster-friendly-Snakemake.html#rules-from-a-config-file",
    "title": "Cluster-friendly Snakemake",
    "section": "Rules from a config file",
    "text": "Rules from a config file\n\nPassing a config file to Snakemake\nIt’s recommended to provide rule-specific resources in a config file. Config files for Snakemake are .yaml format, and they should be specified when running Snakemake from the command line, as such:\nsnakemake --profile ~/.config/snakemake/slurm --config Snakemake_config.yaml\n\n\nUsing a config for resources\nOnce a config file has been provided to Snakemake, it will recognize the variable config in your snakefile, which is a Python dictionary.\nIf your config file looks like this:\nparallelizable_job_threads: 8\nparallelizable_job_mem_mb: 8000\nYour Snakemake rule can be written as such:\nrule parallelizable_job:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    resources:\n        partition=\"&lt;partition_name&gt;\",\n        mem_mb=config.get(\"parallelizable_job_mem_mb\"), # 8 GB\n        runtime=int(2*60), # min, or 2 hours\n        slurm=\"mail-user=&lt;email_address&gt;@colorado.edu\"\n    threads: config.get(\"parallelizable_job_threads\")\n    shell:\n        \"\"\"\n        run_parallelizable_task --threads {threads} --max_memory {resources.mem_mb}\n        \"\"\"\n\n\nSanitizing config resources\nI personally like to set default resources for each rule in my Snakefile and sanitize the config resources. I’m not sure if it’s “best practices”, but it feels safer. For example, if I accidentally delete the memory param from my config, I don’t want the Snakemake to instead pass the default memory param to Slurm.\n\nStory time - don’t forget the resource requirements!!\nI once left out some important resource requirements, leading to crashing two nodes of a compute cluster. When reconstructing genomes from metagenomes (MAGs) using MetaSPAdes (which needed ~150GB per sample), I accidentally deleted the memory requirement in my config file, and Snakemake submitted 50 jobs to Slurm, requesting the default of 8GB per sample. Slurm scheduled all of these jobs on two compute nodes with 500GB total RAM, thinking I needed ~400GB total, when I really needed ~7500GB, and I crashed these compute nodes and received a scary email from the cluster admins. Learn from my mistakes!\n\n\nHow I avoid issues\nI write functions such as this one to handle the resources I’m passing to each rule:\n\ndef get_threads(rule_default, config, rule_name):\n    config_param = config.get(f\"{rule_name}_threads\")\n    if config_param is not None:\n        return int(config_param)\n\n    return rule_default\n\nAnd my rule would look like this:\nrule my_rule:\n    output:\n        \"out_file.txt\"\n    threads: get_threads(rule_default=1, config, \"my_rule\")\n    shell:\n        \"\"\"\n        do things...\n        \"\"\"\nThis function looks for rule-specific threads in the config, formatted as “_threads”. If that exists, it will use the rule-specific threads from the config, but if not, we already have a default rule-specific number of threads specified. I like this structure because it keeps my config file concise while still maintaining the ability to alter rules’ resources.\nIf you have a better way to do this, feel free to open an issue/pull request, and I can add it!"
  },
  {
    "objectID": "quarto/About-Snakemake.html",
    "href": "quarto/About-Snakemake.html",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake is a workflow management tool. That’s fancy words for “it makes your life easier when you have lots of different compute jobs to run.”\nAt a very surface level, Snakemake looks for the output files you need, and if they aren’t there, it figures out all the steps that need to be run in order to get those files.\nConsider the following example. If snakemake can’t find output file, it will run process data on the input data.\n\n\n\n\nflowchart LR\n  B[process data] --&gt; C(output file)\n  A(input data) --&gt; B[process data]\n  \n\n\n\n\n\n\n\nSounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and Snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but Snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier.\n\n\n\n\n\nSnakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  },
  {
    "objectID": "quarto/About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "href": "quarto/About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "title": "About Snakemake",
    "section": "",
    "text": "Sounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and Snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but Snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier."
  },
  {
    "objectID": "quarto/About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "href": "quarto/About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  }
]