[
  {
    "objectID": "About-Snakemake.html",
    "href": "About-Snakemake.html",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake is a workflow management tool. That’s fancy words for “it makes your life easier when you have lots of different compute jobs to run.”\nAt a very surface level, Snakemake looks for the output files you need, and if they aren’t there, it figures out all the steps that need to be run in order to get those files.\nConsider the following example. If snakemake can’t find output file, it will run process data on the input data.\n\n\n\n\nflowchart LR\n  B[process data] --&gt; C(output file)\n  A(input data) --&gt; B[process data]\n  \n\n\n\n\n\n\n\nSounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier.\n\n\n\n\n\nSnakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  },
  {
    "objectID": "About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "href": "About-Snakemake.html#why-do-i-need-a-workflow-manager",
    "title": "About Snakemake",
    "section": "",
    "text": "Sounds pretty simple, right? You might be thinking “I could do that, why do I need Snakemake?”\nTo answer that, Snakemake becomes more useful as (1) The number of steps increases, and (2) the number of times you have to run each step increases. Consider the following analysis, where you have 4 samples, and 2 steps for each sample.\n\n\n\n\nflowchart LR\n  B2[step 2] --&gt; C(sample 1 output)\n  B1[step 1] --&gt; B2[step 2]\n  A(sample 1 input) --&gt; B1[step 1]\n\n  E2[step 2] --&gt; F(sample 2 output)\n  E1[step 1] --&gt; E2[step 2]\n  D(sample 2 input) --&gt; E1[step 1]\n  \n  H2[step 2] --&gt; I(sample 3 output)\n  H1[step 1] --&gt; H2[step 2]\n  G(sample 3 input) --&gt; H1[step 1]\n\n  K2[step 2] --&gt; L(sample 4 output)\n  K1[step 1] --&gt; K2[step 2]\n  J(sample 4 input) --&gt; K1[step 1]\n\n\n\n\n\nThis is going to require more of your personal time to run, compared to the first example, and snakemake may be more useful. For example, if each step takes 2-4 hours, you’d have to check the output after 4 hours and run the next step, but snakemake can automatically start running step 1 for each sample once step 2 is done.\nMore realistically, think about a research study that generates metagenomic sequencing data, where you may have 100 samples, that each need to be run through 10 steps. As things scale up, workflow management just makes your life easier."
  },
  {
    "objectID": "About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "href": "About-Snakemake.html#other-attributes-that-make-snakemake-nice",
    "title": "About Snakemake",
    "section": "",
    "text": "Snakemake can also manage software environments. This comes in useful for conflicting software requirements. For example, imagine step 1 needs Python 3.9, but step 2 only works with Python 3.5. Snakemake can build separate Conda environments for each step, and it provides a framework to keep track of these software environments (aiding reproducibility). If you’ve managed these Conda environments, you can send your snakefile to someone else, and Snakemake will build the software environments for them, so they don’t have to worry about it!\n\n\n\nSnakemake is also nice for parameter space exploration. Imagine that you want to try passing different parameters to step 1. You can make these changes, and snakemake will rerun all downstream steps. There are also ways to easily expand the “parameter space” of your analyses via Snakemake. Sticking with the metagenomics example, you may want to see if trimming your sequencing data at multiple different base pair positions (0, 5, 10, 15, and 20), it’s not too difficult to extend Snakemake to do this."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Effective use of Snakemake for workflow management on high performance compute clusters",
    "section": "",
    "text": "Overview\nWelcome! This is a learning resource on how to effectively use the workflow management tool Snakemake to make your life easier when processing large amounts of data on high performance compute clusters.\n\n\nGoals\nHopefully, this resource will help you:\n\nUnderstand why workflow management tools are useful\nCreate generalizable workflows with Snakemake\nCreate reproducible workflows with Snakemake\nCreate Snakemake workflows that run on compute clusters in an efficient manner (parallelized job submission and resource management)\n\n\n\nApplications to shotgun metagenomics\nSome examples will be focused on dealing with high-throughput sequencing data, such as metagenomics, since this is what I (and the original audience of this resource) work with on a daily basis. However, the contents and principles still apply to other data types.\nA primer on shotgun metagenomic data:\n\nThe Fastq format is how we store sequencing data. It’s a (sometimes compressed) text file with the genome sequence and quality of the sequence.\nIf any of these concepts are unfamiliar to your, or if they spark an interest in working with metagenomic data:\n\nThomas Sharpton has written An introduction to the analysis of shotgun metagenomic data, which is a nice overview of many data processing steps.\nAdditionally, the Metagenomics Wiki explains many of the tools, as well as how to use them."
  },
  {
    "objectID": "Snakemake-Essentials.html",
    "href": "Snakemake-Essentials.html",
    "title": "Snakemake Essentials",
    "section": "",
    "text": "Installation instructions can be found on the Snakemake documentation page here. In short, you want to install conda and mamba-forge, then run:\nmamba create -c conda-forge -c bioconda -n snakemake_env snakemake"
  },
  {
    "objectID": "Snakemake-Essentials.html#what-is-rule-all",
    "href": "Snakemake-Essentials.html#what-is-rule-all",
    "title": "Snakemake Essentials",
    "section": "What is “rule all”?",
    "text": "What is “rule all”?\nAt this point, you may be asking “Wait! You said I should have a rule for each step! What’s this rule all mess??”\nrule all is how we specify the output files we want. We specify our target files using the input to rule all. In a scientific case, consider these the input to the paper you’ll write from your analysis.\nSnakemake is tracking what rules need to be run in order to generate the inputs for other rules, so it will track that - rule run_multiQC’s outputs -&gt; rule all’s inputs - rule run_fastQC’s outputs -&gt; rule run_multiQC’s inputs - rule trim_fastq’s outputs -&gt; rule run_fastQC’s inputs\nTherefore, it knows that in order to have the final files, it will use rule trim_fastq -&gt; rule run_fastQC -&gt; rule run_multiQC -&gt; rule all\n\n\n\n\nflowchart LR\n  C[run_multiQC] --&gt; D(all)\n  B[run_fastQC] --&gt; C[run_multiQC]\n  A[trim_fastq] --&gt; B[run_fastQC]"
  },
  {
    "objectID": "Snakemake-Essentials.html#structure-of-rules",
    "href": "Snakemake-Essentials.html#structure-of-rules",
    "title": "Snakemake Essentials",
    "section": "Structure of rules",
    "text": "Structure of rules\nRules provide crucial information to Snakemake, such as a step’s inputs, output, and the command to run. These are the bare bones of each rule, but as we develop more, we will start to also include aspects of each rule, including the Conda environment, resource requirements (time and memory), and other parameters.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    shell:\n        \"\"\"\n        # do things to input file to make ouput file\n        \"\"\"\nInstead of shell:, users can also use run: which will run Python code.\nrule do_things:\n    input:\n        \"input_file\"\n    output:\n        \"output_file\"\n    run:\n        # python things ..."
  },
  {
    "objectID": "Snakemake-Essentials.html#inputs-and-outputs",
    "href": "Snakemake-Essentials.html#inputs-and-outputs",
    "title": "Snakemake Essentials",
    "section": "Inputs and outputs",
    "text": "Inputs and outputs\nSnakemake traces the inputs and outputs for each rule to know what rules need to be run (and what order to run them in). These are specified very explicitly in the rule, using input: and output:, followed by and indented, comma-separated list, with one entry per line. These can also be named in the list. Another great attribute of snakemake is that these can be referenced in the command it runs.\n\nExamples\n\n1 input, 1 output\nrule rename_file:\n    input:\n        \"old_name.txt\"\n    output:\n        \"new_name.txt\"\n    shell:\n        \"\"\"\n        mv {input} {output}\n        # same as running\n        # mv old_name.txt new_name.txt\n        \"\"\"\n\n\n2 named inputs, 2 named outputs\nrule rename_multiple_files:\n    input:\n        file_1=\"first_file.txt\",\n        file_2=\"second_file.txt\"\n    output:\n        file_1=\"file_1.txt\",\n        file_2=\"file_2.txt\"\n    shell:\n        \"\"\"\n        mv {input.file_1} {output.file_1}\n        mv {input.file_2} {output.file_2}\n        \"\"\""
  },
  {
    "objectID": "Snakemake-Essentials.html#example-with-trimming-reads",
    "href": "Snakemake-Essentials.html#example-with-trimming-reads",
    "title": "Snakemake Essentials",
    "section": "Example with trimming reads",
    "text": "Example with trimming reads\n\nSimple\nImagine you want to trim one fastq file (10 base pairs from the beginning, 5 base pairs from the end) using SeqTK. This is what a very simple snakefile could look like:\nrule all:\n    input:\n        \"trimmed_reads.fq\"\n\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\n\n\nChaining rules\nNow, imagine the raw reads are compressed. We want to unzip them, trim the reads, and recompress them. You could do this in 1 step, but let’s break it up for the sake of learning. That would look like this:\nrule all:\n    input:\n        \"trimmed_reads.fq.gz\"\n\nrule unzip_fastq:\n    input:\n        \"raw_reads.fq.gz\"\n    output:\n        \"raw_reads.fq\"\n    shell:\n        \"\"\"\n        gunzip {input}\n        \"\"\"\n\nrule trim_fastq:\n    input:\n        \"raw_reads.fq\"\n    output:\n        \"trimmed_reads.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b 10 -e 5 {input} &gt; {output}\n        \"\"\"\n\nrule zip_trimmed:\n    input:\n        \"trimmed_reads.fq\"\n    output:\n        \"trimmed_reads.fq.gz\"\n    shell:\n        \"\"\"\n        gzip {input}\n        \"\"\"\nWhich would create a workflow like this:\n\n\n\n\nflowchart LR\n  C[zip_trimmed] --&gt; D(all)\n  B[trim_fastq] --&gt; C[zip_trimmed]\n  A[unzip_fastq] --&gt; B[trim_fastq]\n\n\n\n\n\n(We will build on this example)"
  },
  {
    "objectID": "Snakemake-Essentials.html#about-wildcards",
    "href": "Snakemake-Essentials.html#about-wildcards",
    "title": "Snakemake Essentials",
    "section": "About wildcards",
    "text": "About wildcards\nWildcards are a big part of how we can expand and generalize how our snakemake pipeline works. Consider a wildcards to be a list of values for which you want to run a snakemake rule multiple times. This is a bit like a for loop:\n\nfor value in wildcards: \n    run rule..."
  },
  {
    "objectID": "Snakemake-Essentials.html#syntax",
    "href": "Snakemake-Essentials.html#syntax",
    "title": "Snakemake Essentials",
    "section": "Syntax",
    "text": "Syntax\n\nOne wildcard\nWe can expand a rule using wildcards by expand()ing the input to a rule that requires this rule’s output. For example:\nrule all:\n    input:\n        expand(\"file_{sample}.txt\",\n               sample=[\"1\",\"2\",\"3\"])\n\nrule create_file:\n    output:\n        \"file_{sample}.txt\"\n    shell:\n        \"\"\"\n        touch {output}\n        \"\"\"\nIn this case, Snakemake will create a workflow that looks like this:\n\n\n\n\nflowchart LR\n  A[file_1] --&gt; F(all)\n  B[file_2] --&gt; F(all)\n  C[file_3] --&gt; F(all)\n\n\n\n\n\n\n\nAbout the syntax\nAll you need for a wildcard is a list, which you pass to the expand() function. The first argument in the expand function is your filepath string, with the wildcard in {curly brackets}. Then, you pass the list to use for expanding the filepath. In the previous example, the sample=[\"1\",\"2\",\"3\"] defines the values of the wildcard sample in file_{sample}.txt.\n\n\nMultiple wildcards\nThis doesn’t have to be sample, and you can have multiple wildcards in an expand() function. Imagine you’re plating a 5-course meal for 3 people - your snakefile would look like this:\ncourse_numbers=[\"1\",\"2\",\"3\",\"4\",\"5\"]\npeople=[\"John\", \"Madi\", \"Casey\"]\n\nrule all:\n    expand(\"course_{course_number}_for_{person}.plate\",\n           course_number=course_numbers,\n           person=people)\n\nrule make_food:\n    ouput:\n        \"course_{course_number}_for_{person}.plate\"\n    shell:\n        \"\"\"\n        make food...\n        \"\"\""
  },
  {
    "objectID": "Snakemake-Essentials.html#wildcards-to-run-snakemake-rules-for-each-sample",
    "href": "Snakemake-Essentials.html#wildcards-to-run-snakemake-rules-for-each-sample",
    "title": "Snakemake Essentials",
    "section": "Wildcards to run snakemake rules for each sample",
    "text": "Wildcards to run snakemake rules for each sample\nThis is a very common use for wildcards! We often use wildcards we want to run the same rules for each sample. This is one of the ways Snakemake starts to shine. If you have a metadata file with 500 sample IDs, you can read that list of sample IDs into Snakemake using Python/Pandas, then run your snakemake pipeline for all samples. This is what that looks like:\nimport pandas as pd\nmetadata = pd.read_csv(\"metadata.csv\")\nsamples = metadata[\"SampleID\"] # Samples are in a column named SampleID\n\nrule all:\n    input:\n        expand(\"file_{sample}.txt\",\n               sample=samples)\n\nrule create_file:\n    output:\n        \"file_{sample}.txt\"\n    shell:\n        \"\"\"\n        touch {output}\n        \"\"\""
  },
  {
    "objectID": "Snakemake-Essentials.html#wildcards-for-parameter-exploration",
    "href": "Snakemake-Essentials.html#wildcards-for-parameter-exploration",
    "title": "Snakemake Essentials",
    "section": "Wildcards for parameter exploration",
    "text": "Wildcards for parameter exploration\nAnother common use of wildcards is to explore the effects of using different parameters in your analysis.\nLet’s look back to our example of trimming sequencing reads. If we wanted to look at changes in read quality if we trim each sample’s reads at different base positions, this is what our snakefile could look like:\nimport pandas as pd\nmetadata = pd.read_csv(\"metadata.csv\")\nsamples = metadata[\"SampleID\"] # Samples are in a column named SampleID\n\nread_trim_positions = [\"0\",\"10\",\"20\"]\nread_trunc_positions = [\"0\",\"10\",\"20\"] \n\nrule all:\n    input:\n        \"multiqc_report/multiqc_report.html\"\n\n# Trim fastq files with varying parameters\nrule trim_fastq:\n    input:\n        \"raw_reads_{sample}.fq\"\n    output:\n        \"trimmed_reads_{sample}_b{start}_e{end}.fq\"\n    shell:\n        \"\"\"\n        seqtk trimfq -b {wildcards.start} -e {wildcards.end} {input} &gt; {output}\n        \"\"\"\n\n# Run each of the trimmed fastq files through fastQC for quality control\nrule run_fastQC:\n    input:\n        \"trimmed_reads_{sample}_b{start}_e{end}.fq\"\n    output:\n        \"trimmed_reads_{sample}_b{start}_e{end}_fastqc.zip\"\n    shell:\n        \"\"\"\n        fastqc {input} -o .\n        \"\"\"\n\n# Aggregate fastQC reports using MultiQC\nrule run_multiQC:\n    input:\n        ### LOOK HERE, THIS IS OUR EXPAND COMMAND #################\n        expand(\"trimmed_reads_{sample}_b{start}_e{end}_fastqc.zip\",\n                sample=samples,\n                start=read_trim_positions,\n                end=read_trunc_positions)\n    output:\n        \"multiqc_report/multiqc_report.html\"\n    shell:\n        \"\"\"\n        multiqc . -o multiqc_report\n        \"\"\""
  }
]